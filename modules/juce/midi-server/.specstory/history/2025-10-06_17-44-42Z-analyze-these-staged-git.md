<!-- Generated by SpecStory -->

# 2025-10-06 17:44:42Z

<!-- Claude Code Session 9c39fe76-4f42-4f41-b58d-1445c82e9a93 (2025-10-06_17-44-42Z) -->

_**User**_

Analyze these staged git changes and output ONLY a single-line conventional commit message (type(scope): description). No explanations.
diff --git a/modules/juce/midi-server/.gitignore b/modules/juce/midi-server/.gitignore
new file mode 100644
index 0000000..990936c
--- /dev/null
+++ b/modules/juce/midi-server/.gitignore
@@ -0,0 +1,2 @@
+.idea/
+cmake-build-debug/
diff --git a/modules/juce/midi-server/NetworkMidiServer.cpp b/modules/juce/midi-server/NetworkMidiServer.cpp
index 5ac7aa5..ca6e1a6 100644
--- a/modules/juce/midi-server/NetworkMidiServer.cpp
+++ b/modules/juce/midi-server/NetworkMidiServer.cpp
@@ -614,32 +614,51 @@ private:
 
         // Network mesh status
         server->Get("/network/mesh", [this](const httplib::Request&, httplib::Response& res) {
-            JsonBuilder json;
-            json.startObject();
+            try {
+                std::cerr << "DEBUG: /network/mesh: Starting" << std::endl;
+                JsonBuilder json;
+                json.startObject();
 
-            if (meshManager) {
-                auto stats = meshManager->getStatistics();
-                json.key("connected_nodes").value((int)stats.connectedNodes)
-                    .key("total_nodes").value((int)stats.totalNodes)
-                    .key("total_devices").value(stats.totalDevices);
-
-                json.key("nodes").startArray();
-                auto nodes = meshManager->getConnectedNodes();
-                for (const auto& node : nodes) {
-                    json.startObject()
-                        .key("uuid").value(node.uuid.toString().toStdString())
-                        .key("name").value(node.name.toStdString())
-                        .key("ip").value(node.ipAddress.toStdString())
-                        .key("http_port").value(node.httpPort)
-                        .key("udp_port").value(node.udpPort)
-                        .key("devices").value(node.deviceCount)
-                        .endObject();
+                if (meshManager) {
+                    std::cerr << "DEBUG: /network/mesh: Getting statistics" << std::endl;
+                    auto stats = meshManager->getStatistics();
+                    std::cerr << "DEBUG: /network/mesh: Got statistics" << std::endl;
+                    json.key("connected_nodes").value((int)stats.connectedNodes)
+                        .key("total_nodes").value((int)stats.totalNodes)
+                        .key("total_devices").value(stats.totalDevices);
+
+                    std::cerr << "DEBUG: /network/mesh: Getting connected nodes" << std::endl;
+                    json.key("nodes").startArray();
+                    auto nodes = meshManager->getConnectedNodes();
+                    std::cerr << "DEBUG: /network/mesh: Got " << nodes.size() << " nodes" << std::endl;
+                    for (const auto& node : nodes) {
+                        std::cerr << "DEBUG: /network/mesh: Processing node " << node.name.toStdString() << std::endl;
+                        json.startObject()
+                            .key("uuid").value(node.uuid.toString().toStdString())
+                            .key("name").value(node.name.toStdString())
+                            .key("ip").value(node.ipAddress.toStdString())
+                            .key("http_port").value(node.httpPort)
+                            .key("udp_port").value(node.udpPort)
+                            .key("devices").value(node.deviceCount)
+                            .endObject();
+                        std::cerr << "DEBUG: /network/mesh: Finished node " << node.name.toStdString() << std::endl;
+                    }
+                    json.endArray();
+                } else {
+                    json.key("error").value(std::string("Mesh manager not initialized"));
                 }
-                json.endArray();
-            }
 
-            json.endObject();
-            res.set_content(json.toString(), "application/json");
+                std::cerr << "DEBUG: /network/mesh: Building JSON" << std::endl;
+                json.endObject();
+                std::cerr << "DEBUG: /network/mesh: Sending response" << std::endl;
+                res.set_content(json.toString(), "application/json");
+                res.status = 200;
+                std::cerr << "DEBUG: /network/mesh: Done" << std::endl;
+            } catch (const std::exception& e) {
+                std::cerr << "Error in /network/mesh: " << e.what() << std::endl;
+                res.set_content(std::string("Error: ") + e.what(), "text/plain");
+                res.status = 500;
+            }
         });
 
         // Router statistics
@@ -667,6 +686,88 @@ private:
             json.endObject();
             res.set_content(json.toString(), "application/json");
         });
+
+        // Network handshake endpoint (for mesh connection establishment)
+        server->Post("/network/handshake", [this](const httplib::Request& req, httplib::Response& res) {
+            try {
+                // Parse incoming handshake request
+                // Expected format: {"node_id":"...", "node_name":"...", "udp_endpoint":"...", "version":"..."}
+
+                std::string nodeIdStr, nodeName, udpEndpoint, version;
+
+                // Simple JSON parsing (extract values)
+                auto body = req.body;
+                size_t pos;
+
+                // Extract node_id
+                pos = body.find("\"node_id\":\"");
+                if (pos != std::string::npos) {
+                    size_t start = pos + 11;
+                    size_t end = body.find("\"", start);
+                    nodeIdStr = body.substr(start, end - start);
+                }
+
+                // Extract node_name
+                pos = body.find("\"node_name\":\"");
+                if (pos != std::string::npos) {
+                    size_t start = pos + 13;
+                    size_t end = body.find("\"", start);
+                    nodeName = body.substr(start, end - start);
+                }
+
+                // Extract udp_endpoint
+                pos = body.find("\"udp_endpoint\":\"");
+                if (pos != std::string::npos) {
+                    size_t start = pos + 16;
+                    size_t end = body.find("\"", start);
+                    udpEndpoint = body.substr(start, end - start);
+                }
+
+                // Extract version
+                pos = body.find("\"version\":\"");
+                if (pos != std::string::npos) {
+                    size_t start = pos + 11;
+                    size_t end = body.find("\"", start);
+                    version = body.substr(start, end - start);
+                }
+
+                std::cout << "Handshake request from: " << nodeName
+                          << " (UUID: " << nodeIdStr << ")"
+                          << " UDP: " << udpEndpoint << std::endl;
+
+                // Build handshake response with our node info and device list
+                JsonBuilder json;
+                json.startObject()
+                    .key("node_id").value(identity.getNodeId().toString().toStdString())
+                    .key("node_name").value(identity.getNodeName().toStdString())
+                    .key("udp_endpoint").value(std::to_string(udpPort))
+                    .key("version").value(std::string("1.0"));
+
+                // Add device list
+                if (deviceRegistry) {
+                    auto devices = deviceRegistry->getLocalDevices();
+                    json.key("devices").startArray();
+                    for (const auto& device : devices) {
+                        json.startObject()
+                            .key("id").value((int)device.id)
+                            .key("name").value(device.name.toStdString())
+                            .key("type").value(device.type.toStdString())
+                            .endObject();
+                    }
+                    json.endArray();
+                }
+
+                json.endObject();
+
+                res.set_content(json.toString(), "application/json");
+                res.status = 200;
+
+            } catch (const std::exception& e) {
+                std::cerr << "Handshake error: " << e.what() << std::endl;
+                res.set_content(std::string("Handshake failed: ") + e.what(), "text/plain");
+                res.status = 500;
+            }
+        });
     }
 
     const NetworkMidi::NodeIdentity& identity;
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/COVERAGE_REPORT.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/COVERAGE_REPORT.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/COVERAGE_REPORT.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/COVERAGE_REPORT.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/FINAL-STATUS.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/FINAL-STATUS.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/FINAL-STATUS.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/FINAL-STATUS.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/IMPLEMENTATION_SUMMARY.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/IMPLEMENTATION_SUMMARY.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/IMPLEMENTATION_SUMMARY.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/IMPLEMENTATION_SUMMARY.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/INTEGRATION_TEST_RESULTS.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/INTEGRATION_TEST_RESULTS.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/INTEGRATION_TEST_RESULTS.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/INTEGRATION_TEST_RESULTS.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/implementation-complete.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/implementation-complete.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/implementation-complete.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/implementation-complete.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/phase3-report.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/phase3-report.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/phase3-report.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/phase3-report.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/test-results.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/test-results.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/test-results.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/test-results.md
diff --git a/modules/juce/midi-server/docs/1.0/implementation/mesh/workplan.md b/modules/juce/midi-server/docs/1.0/mesh/implementation/workplan.md
similarity index 100%
rename from modules/juce/midi-server/docs/1.0/implementation/mesh/workplan.md
rename to modules/juce/midi-server/docs/1.0/mesh/implementation/workplan.md
diff --git a/modules/juce/midi-server/docs/1.0/seda/implementation/workplan.md b/modules/juce/midi-server/docs/1.0/seda/implementation/workplan.md
new file mode 100644
index 0000000..d3c611e
--- /dev/null
+++ b/modules/juce/midi-server/docs/1.0/seda/implementation/workplan.md
@@ -0,0 +1,1724 @@
+# SEDA Implementation Workplan
+
+**Document Version**: 1.0
+**Date**: 2025-10-06
+**Status**: Ready for Execution
+**Related Design**: [design.md](../planning/design.md)
+
+---
+
+## Table of Contents
+
+1. [Executive Summary](#executive-summary)
+2. [Strategic Decision](#strategic-decision)
+3. [Approach A: Mutex Fixes (Recommended Start)](#approach-a-mutex-fixes-recommended-start)
+4. [Approach B: Full SEDA Migration](#approach-b-full-seda-migration)
+5. [Decision Criteria](#decision-criteria)
+6. [Integration with Current Phase 6](#integration-with-current-phase-6)
+7. [Testing & Validation Strategy](#testing--validation-strategy)
+8. [Timeline & Resources](#timeline--resources)
+9. [Risk Assessment](#risk-assessment)
+10. [Rollback Procedures](#rollback-procedures)
+
+---
+
+## Executive Summary
+
+This workplan provides two parallel implementation strategies for resolving the mutex-based deadlock potential in `NetworkConnection`:
+
+- **Approach A (Mutex Fixes)**: Quick, low-risk fixes to eliminate callback-under-lock and race conditions - **2-4 hours**
+- **Approach B (Full SEDA)**: Complete architectural migration to event-driven stages - **8-14 hours**
+
+**Recommendation**: Start with Approach A to eliminate immediate deadlock risks, then collect production metrics to determine if full SEDA migration is warranted.
+
+### Key Metrics
+
+| Metric | Current (Mutex) | After Fixes | Full SEDA | Target |
+|--------|----------------|-------------|-----------|--------|
+| Deadlock Risk | Medium | Low | Zero | Zero |
+| Query Latency | ~100ns-10ms | ~100ns | ~10ns (atomic) | <1ms |
+| Implementation Time | 0h | 2-4h | 8-14h | Minimize |
+| Code Complexity | Low | Low | Medium-High | Balance |
+| Maintainability | Good | Good | Medium | High |
+
+---
+
+## Strategic Decision
+
+### Decision Framework
+
+```
+┌─────────────────────────────────────────────────────────┐
+│                  Decision Tree                           │
+├─────────────────────────────────────────────────────────┤
+│                                                           │
+│  Are deadlocks occurring in production?                 │
+│           │                                              │
+│           ├── NO ──> Start with Approach A (Mutex Fixes)│
+│           │          Monitor for 1-2 weeks               │
+│           │          Collect metrics                     │
+│           │                                              │
+│           └── YES ──> Skip to Approach B (Full SEDA)    │
+│                       High priority migration            │
+│                                                           │
+│  After Approach A:                                       │
+│  ├── Metrics show <1% contention ──> DONE              │
+│  ├── Metrics show >5% contention ──> Proceed to B      │
+│  └── Deadlocks still occur ──> Emergency B              │
+│                                                           │
+└─────────────────────────────────────────────────────────┘
+```
+
+### Current Status Assessment
+
+Based on the design document analysis:
+
+- **Deadlock Evidence**: Potential but not confirmed in production
+- **Contention Evidence**: Not measured
+- **Scale Requirements**: Current mesh typically <10 connections
+- **Performance Requirements**: HTTP queries must complete <100ms
+
+**Recommended Path**: **Approach A first**, with Approach B as planned follow-up if metrics justify it.
+
+---
+
+## Approach A: Mutex Fixes (Recommended Start)
+
+### Overview
+
+Eliminate the three primary deadlock scenarios with minimal code changes:
+
+1. Fix callback-under-lock pattern in `setState()`
+2. Protect `remoteDevices` with proper synchronization
+3. Move callback dispatch to message thread
+4. Add defensive timeouts on mutex acquisitions
+
+**Effort Estimate**: 2-4 hours
+**Risk Level**: Low
+**Rollback Complexity**: Trivial (git revert)
+
+---
+
+### Phase A.1: Fix Callback-Under-Lock
+
+**Objective**: Ensure callbacks are never invoked while holding mutexes.
+
+**Duration**: 30-45 minutes
+
+#### Tasks
+
+1. **Modify `NetworkConnection::setState()`**
+   - File: `/Users/orion/work/ol_dsp-midi-server/modules/juce/midi-server/network/mesh/NetworkConnection.cpp`
+   - Change callback invocation pattern
+
+2. **Implementation**:
+
+```cpp
+// BEFORE (NetworkConnection.cpp:255-271)
+void NetworkConnection::setState(State newState)
+{
+    State oldState = currentState.load();
+    // ...
+    currentState = newState;
+    // ...
+    if (onStateChanged) {
+        onStateChanged(oldState, newState);  // ⚠️ UNDER LOCK
+    }
+}
+
+// AFTER
+void NetworkConnection::setState(State newState)
+{
+    State oldState = currentState.load();
+
+    // Update state under lock (if needed)
+    {
+        std::lock_guard<std::mutex> lock(stateMutex);
+        currentState = newState;
+        // ... other state updates ...
+    }
+
+    // Dispatch callback OUTSIDE lock via message thread
+    if (onStateChanged) {
+        auto callback = onStateChanged;  // Copy callback
+        juce::MessageManager::callAsync([callback, oldState, newState]() {
+            callback(oldState, newState);
+        });
+    }
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] `setState()` releases all mutexes before callback invocation
+- [ ] Callbacks dispatched via `juce::MessageManager::callAsync()`
+- [ ] Existing state transition tests still pass
+- [ ] No compiler warnings
+
+#### Testing
+
+```cpp
+// New test: Ensure callbacks are async
+TEST(NetworkConnectionMutexFix, CallbackNotUnderLock) {
+    NetworkConnection conn(testNodeInfo);
+
+    std::atomic<bool> callbackExecuted{false};
+    std::thread::id callbackThread;
+
+    conn.onStateChanged = [&](auto oldState, auto newState) {
+        callbackThread = std::this_thread::get_id();
+        callbackExecuted = true;
+    };
+
+    conn.connect();
+
+    // Wait for callback
+    for (int i = 0; i < 100 && !callbackExecuted; ++i) {
+        juce::MessageManager::getInstance()->runDispatchLoopUntil(10);
+    }
+
+    EXPECT_TRUE(callbackExecuted);
+    EXPECT_NE(callbackThread, std::this_thread::get_id());  // Different thread
+}
+```
+
+---
+
+### Phase A.2: Protect remoteDevices Access
+
+**Objective**: Eliminate Race #3 (device list read without protection).
+
+**Duration**: 45-60 minutes
+
+#### Tasks
+
+1. **Add mutex protection to device list operations**
+   - File: `NetworkConnection.cpp` and `NetworkConnection.h`
+
+2. **Implementation**:
+
+```cpp
+// NetworkConnection.h - Add new mutex
+private:
+    mutable std::mutex devicesMutex;  // Protects remoteDevices
+
+// NetworkConnection.cpp:91-96 - Update getRemoteDevices()
+std::vector<DeviceInfo> NetworkConnection::getRemoteDevices() const
+{
+    std::lock_guard<std::mutex> lock(devicesMutex);
+    return remoteDevices;  // ✅ Now safe
+}
+
+// NetworkConnection.cpp:213-223 - Update performHandshake()
+void NetworkConnection::performHandshake()
+{
+    // ... HTTP request ...
+
+    if (success) {
+        {
+            std::lock_guard<std::mutex> lock(devicesMutex);
+            remoteDevices = parsedDevices;  // ✅ Protected write
+        }
+
+        setState(State::Connected);
+    }
+}
+```
+
+#### Alternative: Use Atomic Shared Pointer (Zero-Copy Reads)
+
+```cpp
+// NetworkConnection.h
+private:
+    std::atomic<std::shared_ptr<const std::vector<DeviceInfo>>> remoteDevicesPtr;
+
+// NetworkConnection.cpp
+std::vector<DeviceInfo> NetworkConnection::getRemoteDevices() const
+{
+    auto devices = remoteDevicesPtr.load(std::memory_order_acquire);
+    return devices ? *devices : std::vector<DeviceInfo>{};
+}
+
+void NetworkConnection::performHandshake()
+{
+    // ... parse devices ...
+
+    auto devices = std::make_shared<std::vector<DeviceInfo>>(parsedDevices);
+    remoteDevicesPtr.store(devices, std::memory_order_release);
+}
+```
+
+**Recommendation**: Use atomic shared pointer for better read performance.
+
+#### Acceptance Criteria
+
+- [ ] All `remoteDevices` reads are protected
+- [ ] All `remoteDevices` writes are protected
+- [ ] No data races detected by ThreadSanitizer
+- [ ] Device list tests pass
+
+---
+
+### Phase A.3: Add Mutex Timeouts (Defensive)
+
+**Objective**: Prevent indefinite blocking in case of unforeseen contention.
+
+**Duration**: 30-45 minutes
+
+#### Tasks
+
+1. **Replace `std::mutex` with `std::timed_mutex`**
+   - File: `NetworkConnection.h`
+
+2. **Implementation**:
+
+```cpp
+// NetworkConnection.h
+private:
+    std::timed_mutex stateMutex;
+    std::timed_mutex heartbeatMutex;
+    std::timed_mutex devicesMutex;
+
+// NetworkConnection.cpp - Example with timeout
+State NetworkConnection::getState() const
+{
+    std::unique_lock<std::timed_mutex> lock(stateMutex, std::chrono::milliseconds(100));
+
+    if (!lock.owns_lock()) {
+        // Log warning - mutex contention detected
+        juce::Logger::writeToLog("WARNING: getState() mutex timeout - possible contention");
+
+        // Return atomic snapshot as fallback
+        return currentState.load(std::memory_order_acquire);
+    }
+
+    return currentState;
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] All mutex acquisitions have 100ms timeout
+- [ ] Timeouts logged to help identify contention
+- [ ] Fallback to atomic reads where possible
+- [ ] No false positives in normal operation
+
+---
+
+### Phase A.4: Fix MeshManager Connection Lifecycle
+
+**Objective**: Resolve Race #4 (connection may be deleted between pool query and dereference).
+
+**Duration**: 45-60 minutes
+
+#### Tasks
+
+1. **Modify `MeshManager::getTotalDeviceCount()`**
+   - File: `/Users/orion/work/ol_dsp-midi-server/modules/juce/midi-server/network/mesh/MeshManager.cpp`
+
+2. **Implementation**:
+
+```cpp
+// BEFORE (MeshManager.cpp:147-166)
+int MeshManager::getTotalDeviceCount() const
+{
+    auto connections = connectionPool.getAllConnections();  // ⚠️ Pool lock released
+
+    for (auto* connection : connections) {
+        if (connection && connection->getState() == NetworkConnection::State::Connected) {
+            total += (int)connection->getRemoteDevices().size();  // ⚠️ May be deleted!
+        }
+    }
+}
+
+// AFTER - Hold pool lock during iteration
+int MeshManager::getTotalDeviceCount() const
+{
+    return connectionPool.withAllConnections([](const auto& connections) {
+        int total = 0;
+        for (const auto* connection : connections) {
+            if (connection && connection->getState() == NetworkConnection::State::Connected) {
+                total += (int)connection->getRemoteDevices().size();
+            }
+        }
+        return total;
+    });
+}
+
+// ConnectionPool.h - Add withAllConnections helper
+template<typename Func>
+auto withAllConnections(Func&& func) const {
+    std::lock_guard<std::mutex> lock(poolMutex);
+    return func(connections);  // Callback executes under lock
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] Connection pool operations atomic
+- [ ] No use-after-free in iteration
+- [ ] All mesh manager tests pass
+- [ ] No deadlocks when querying pool
+
+---
+
+### Phase A.5: Validation & Testing
+
+**Duration**: 30-45 minutes
+
+#### Validation Checklist
+
+- [ ] Build succeeds with no warnings: `cmake --build build`
+- [ ] All existing unit tests pass: `ctest --test-dir build`
+- [ ] ThreadSanitizer clean: `cmake -DCMAKE_CXX_FLAGS="-fsanitize=thread" ..`
+- [ ] Stress test: 100 threads querying concurrently for 60 seconds
+- [ ] Callback ordering test: Verify message thread execution
+- [ ] Performance baseline: Measure query latency before/after
+
+#### New Tests Required
+
+```cpp
+// Test 1: Concurrent queries with state changes
+TEST(NetworkConnectionMutexFix, ConcurrentQueriesDuringHandshake) {
+    // 50 reader threads + 1 connection thread
+    // Run for 10 seconds, verify no deadlocks
+}
+
+// Test 2: Callback reentrancy safety
+TEST(NetworkConnectionMutexFix, CallbackReentrancySafe) {
+    // Callback calls getRemoteNode() - should not deadlock
+}
+
+// Test 3: Device list consistency
+TEST(NetworkConnectionMutexFix, DeviceListReadDuringWrite) {
+    // Read devices while handshake updates list
+    // Verify no crashes, no partial data
+}
+
+// Test 4: Pool iteration safety
+TEST(MeshManagerFix, PoolIterationDuringDisconnect) {
+    // Disconnect connection while getTotalDeviceCount() iterating
+    // Verify no use-after-free
+}
+```
+
+---
+
+## Approach B: Full SEDA Migration
+
+### Overview
+
+Complete architectural transformation to staged event-driven design with zero shared mutexes.
+
+**Effort Estimate**: 8-14 hours
+**Risk Level**: Medium
+**Rollback Complexity**: Moderate (feature branch required)
+
+**Note**: Only proceed if Approach A proves insufficient OR if production metrics show >5% mutex contention.
+
+---
+
+### Phase B.1: Infrastructure Setup
+
+**Objective**: Create command queue and worker thread infrastructure.
+
+**Duration**: 2-3 hours
+
+#### Tasks
+
+1. **Create command hierarchy**
+   - New file: `/Users/orion/work/ol_dsp-midi-server/modules/juce/midi-server/network/mesh/NetworkCommands.h`
+
+```cpp
+#pragma once
+
+#include <juce_core/juce_core.h>
+#include <juce_events/juce_events.h>
+#include <memory>
+#include <vector>
+
+namespace NetworkMidi {
+namespace Commands {
+
+// Base command
+struct Command {
+    enum Type {
+        Connect,
+        Disconnect,
+        CheckHeartbeat,
+        NotifyHeartbeat,
+        SendMidi,
+        GetRemoteNode,
+        GetDevices,
+        Shutdown
+    };
+
+    Type type;
+    virtual ~Command() = default;
+};
+
+// Simple commands
+struct ConnectCommand : Command {
+    ConnectCommand() { type = Connect; }
+};
+
+struct DisconnectCommand : Command {
+    DisconnectCommand() { type = Disconnect; }
+};
+
+struct CheckHeartbeatCommand : Command {
+    CheckHeartbeatCommand() { type = CheckHeartbeat; }
+};
+
+struct NotifyHeartbeatCommand : Command {
+    NotifyHeartbeatCommand() { type = NotifyHeartbeat; }
+};
+
+// Query commands with synchronous response
+struct GetRemoteNodeQuery : Command {
+    juce::WaitableEvent responseReady;
+    NodeInfo result;
+
+    GetRemoteNodeQuery() { type = GetRemoteNode; }
+};
+
+struct GetDevicesQuery : Command {
+    juce::WaitableEvent responseReady;
+    std::vector<DeviceInfo> result;
+
+    GetDevicesQuery() { type = GetDevices; }
+};
+
+struct ShutdownCommand : Command {
+    ShutdownCommand() { type = Shutdown; }
+};
+
+} // namespace Commands
+} // namespace NetworkMidi
+```
+
+2. **Create command queue**
+   - New file: `/Users/orion/work/ol_dsp-midi-server/modules/juce/midi-server/network/mesh/CommandQueue.h`
+
+```cpp
+#pragma once
+
+#include "NetworkCommands.h"
+#include <juce_events/juce_events.h>
+#include <deque>
+#include <mutex>
+
+namespace NetworkMidi {
+
+class CommandQueue {
+public:
+    void push(std::unique_ptr<Commands::Command> cmd) {
+        {
+            std::lock_guard<std::mutex> lock(queueMutex);
+            queue.push_back(std::move(cmd));
+        }
+        queueEvent.signal();
+    }
+
+    std::unique_ptr<Commands::Command> pop(int timeoutMs) {
+        if (!queueEvent.wait(timeoutMs)) {
+            return nullptr;
+        }
+
+        std::lock_guard<std::mutex> lock(queueMutex);
+        if (queue.empty()) {
+            return nullptr;
+        }
+
+        auto cmd = std::move(queue.front());
+        queue.pop_front();
+
+        if (!queue.empty()) {
+            queueEvent.signal();
+        }
+
+        return cmd;
+    }
+
+    size_t size() const {
+        std::lock_guard<std::mutex> lock(queueMutex);
+        return queue.size();
+    }
+
+private:
+    mutable std::mutex queueMutex;
+    std::deque<std::unique_ptr<Commands::Command>> queue;
+    juce::WaitableEvent queueEvent;
+};
+
+} // namespace NetworkMidi
+```
+
+3. **Create worker thread**
+   - New file: `/Users/orion/work/ol_dsp-midi-server/modules/juce/midi-server/network/mesh/ConnectionWorker.h`
+
+```cpp
+#pragma once
+
+#include "CommandQueue.h"
+#include "NetworkCommands.h"
+#include <juce_core/juce_core.h>
+#include <atomic>
+
+namespace NetworkMidi {
+
+class ConnectionWorker : public juce::Thread {
+public:
+    ConnectionWorker(CommandQueue& queue);
+    ~ConnectionWorker() override;
+
+    void run() override;
+
+    // Atomic snapshots for fast queries
+    State getStateSnapshot() const {
+        return stateSnapshot.load(std::memory_order_acquire);
+    }
+
+    int64_t getHeartbeatSnapshot() const {
+        return heartbeatSnapshot.load(std::memory_order_acquire);
+    }
+
+private:
+    CommandQueue& commandQueue;
+
+    // Worker-owned state (no mutexes needed)
+    State currentState{State::Disconnected};
+    int64_t lastHeartbeatTime{0};
+    NodeInfo remoteNodeInfo;
+    std::vector<DeviceInfo> remoteDevices;
+    std::unique_ptr<httplib::Client> httpClient;
+
+    // Atomic snapshots for queries
+    std::atomic<State> stateSnapshot{State::Disconnected};
+    std::atomic<int64_t> heartbeatSnapshot{0};
+
+    // Command handlers
+    void processCommand(std::unique_ptr<Commands::Command> cmd);
+    void handleConnect();
+    void handleDisconnect();
+    void handleCheckHeartbeat();
+    void handleNotifyHeartbeat();
+    void handleGetRemoteNodeQuery(Commands::GetRemoteNodeQuery* query);
+    void handleGetDevicesQuery(Commands::GetDevicesQuery* query);
+
+    // Helper methods
+    void updateSnapshots();
+    void dispatchStateChangedCallback(State oldState, State newState);
+    bool performHandshake();
+};
+
+} // namespace NetworkMidi
+```
+
+#### Acceptance Criteria
+
+- [ ] CommandQueue tests pass (push/pop operations)
+- [ ] Worker thread starts and stops cleanly
+- [ ] Command dispatch works (send Connect, verify handleConnect called)
+- [ ] No memory leaks (valgrind clean)
+
+---
+
+### Phase B.2: State Migration
+
+**Objective**: Move state ownership to worker thread, use atomic snapshots for queries.
+
+**Duration**: 2-3 hours
+
+#### Tasks
+
+1. **Modify NetworkConnection to use worker**
+   - File: `NetworkConnection.h`
+
+```cpp
+class NetworkConnection {
+public:
+    NetworkConnection(const NodeInfo& remoteNode);
+    ~NetworkConnection();
+
+    // Public API - dispatch commands
+    void connect();
+    void disconnect();
+
+    // Fast queries via atomic snapshots
+    State getState() const {
+        return worker.getStateSnapshot();
+    }
+
+    int64_t getTimeSinceLastHeartbeat() const {
+        int64_t last = worker.getHeartbeatSnapshot();
+        return juce::Time::getCurrentTime().toMilliseconds() - last;
+    }
+
+    // Complex queries via command queue
+    NodeInfo getRemoteNode() const;
+    std::vector<DeviceInfo> getRemoteDevices() const;
+
+    // Callbacks
+    std::function<void(State, State)> onStateChanged;
+    std::function<void(const juce::String&)> onError;
+
+private:
+    CommandQueue commandQueue;
+    ConnectionWorker worker;
+
+    // NO MUTEXES - all state owned by worker
+};
+```
+
+2. **Implement query methods**
+   - File: `NetworkConnection.cpp`
+
+```cpp
+NetworkConnection::NetworkConnection(const NodeInfo& remoteNode)
+    : worker(commandQueue, remoteNode)
+{
+    worker.startThread();
+}
+
+NetworkConnection::~NetworkConnection()
+{
+    commandQueue.push(std::make_unique<Commands::ShutdownCommand>());
+    worker.stopThread(2000);  // 2 second timeout
+}
+
+void NetworkConnection::connect()
+{
+    commandQueue.push(std::make_unique<Commands::ConnectCommand>());
+}
+
+void NetworkConnection::disconnect()
+{
+    commandQueue.push(std::make_unique<Commands::DisconnectCommand>());
+}
+
+NodeInfo NetworkConnection::getRemoteNode() const
+{
+    auto query = std::make_unique<Commands::GetRemoteNodeQuery>();
+    auto* queryPtr = query.get();
+
+    commandQueue.push(std::move(query));
+    queryPtr->responseReady.wait();
+
+    return queryPtr->result;
+}
+
+std::vector<DeviceInfo> NetworkConnection::getRemoteDevices() const
+{
+    auto query = std::make_unique<Commands::GetDevicesQuery>();
+    auto* queryPtr = query.get();
+
+    commandQueue.push(std::move(query));
+    queryPtr->responseReady.wait();
+
+    return queryPtr->result;
+}
+```
+
+3. **Implement worker command handlers**
+   - File: `ConnectionWorker.cpp`
+
+```cpp
+void ConnectionWorker::processCommand(std::unique_ptr<Commands::Command> cmd)
+{
+    switch (cmd->type) {
+        case Commands::Command::Connect:
+            handleConnect();
+            break;
+
+        case Commands::Command::Disconnect:
+            handleDisconnect();
+            break;
+
+        case Commands::Command::CheckHeartbeat:
+            handleCheckHeartbeat();
+            break;
+
+        case Commands::Command::GetRemoteNode:
+            handleGetRemoteNodeQuery(
+                static_cast<Commands::GetRemoteNodeQuery*>(cmd.get()));
+            break;
+
+        case Commands::Command::GetDevices:
+            handleGetDevicesQuery(
+                static_cast<Commands::GetDevicesQuery*>(cmd.get()));
+            break;
+
+        case Commands::Command::Shutdown:
+            signalThreadShouldExit();
+            break;
+    }
+}
+
+void ConnectionWorker::handleConnect()
+{
+    State oldState = currentState;
+    currentState = State::Connecting;
+    updateSnapshots();
+
+    bool success = performHandshake();
+
+    if (success) {
+        currentState = State::Connected;
+        lastHeartbeatTime = juce::Time::getCurrentTime().toMilliseconds();
+        updateSnapshots();
+
+        dispatchStateChangedCallback(oldState, State::Connected);
+    } else {
+        currentState = State::Failed;
+        updateSnapshots();
+
+        dispatchStateChangedCallback(oldState, State::Failed);
+    }
+}
+
+void ConnectionWorker::handleGetRemoteNodeQuery(Commands::GetRemoteNodeQuery* query)
+{
+    query->result = remoteNodeInfo;
+    query->responseReady.signal();
+}
+
+void ConnectionWorker::handleGetDevicesQuery(Commands::GetDevicesQuery* query)
+{
+    query->result = remoteDevices;
+    query->responseReady.signal();
+}
+
+void ConnectionWorker::updateSnapshots()
+{
+    stateSnapshot.store(currentState, std::memory_order_release);
+    heartbeatSnapshot.store(lastHeartbeatTime, std::memory_order_release);
+}
+
+void ConnectionWorker::dispatchStateChangedCallback(State oldState, State newState)
+{
+    if (onStateChanged) {
+        juce::MessageManager::callAsync([this, oldState, newState]() {
+            if (onStateChanged) {
+                onStateChanged(oldState, newState);
+            }
+        });
+    }
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] All state mutations happen on worker thread
+- [ ] getState() uses atomic snapshot (no blocking)
+- [ ] getRemoteNode() uses query command (blocking)
+- [ ] No mutexes in NetworkConnection class
+- [ ] ThreadSanitizer reports zero data races
+
+---
+
+### Phase B.3: Mutex Removal
+
+**Objective**: Remove all mutex fields from NetworkConnection.
+
+**Duration**: 1-2 hours
+
+#### Tasks
+
+1. **Remove mutex declarations**
+   - File: `NetworkConnection.h`
+
+```cpp
+// REMOVE these lines:
+// std::mutex stateMutex;
+// std::mutex heartbeatMutex;
+// std::mutex messageMutex;
+// std::mutex devicesMutex;
+```
+
+2. **Remove all lock_guard usage**
+   - File: `NetworkConnection.cpp`
+   - Search for `std::lock_guard` and verify all removed
+
+3. **Update HeartbeatMonitor integration**
+   - File: `HeartbeatMonitor.cpp`
+
+```cpp
+void HeartbeatMonitor::checkTimeouts()
+{
+    // Now uses atomic snapshot - no blocking
+    for (auto* connection : connections) {
+        if (connection->getTimeSinceLastHeartbeat() > timeoutMs) {
+            // Post check command instead of direct call
+            connection->checkHeartbeat();
+        }
+    }
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] Zero `std::mutex` or `std::lock_guard` in NetworkConnection
+- [ ] Build succeeds
+- [ ] All tests pass
+- [ ] No performance regression in benchmarks
+
+---
+
+### Phase B.4: Integration Updates
+
+**Objective**: Update MeshManager and HTTP handlers to work with new API.
+
+**Duration**: 2-3 hours
+
+#### Tasks
+
+1. **Update MeshManager**
+   - File: `MeshManager.cpp`
+   - No changes needed if only using public API
+   - Verify `getTotalDeviceCount()` still works
+
+2. **Update HTTP handlers**
+   - File: HTTP server implementation
+   - Verify `/network/mesh` endpoint works
+   - Ensure no blocking on main thread
+
+3. **Update tests**
+   - Modify existing tests to account for async callback dispatch
+   - Add delays/polling where needed
+
+```cpp
+// OLD TEST
+TEST(NetworkConnection, ConnectSetsState) {
+    connection.connect();
+    EXPECT_EQ(connection.getState(), State::Connecting);  // ❌ May be immediate
+}
+
+// NEW TEST
+TEST(NetworkConnection, ConnectSetsState) {
+    connection.connect();
+
+    // Poll for state change
+    for (int i = 0; i < 100; ++i) {
+        if (connection.getState() == State::Connecting) {
+            break;
+        }
+        std::this_thread::sleep_for(std::chrono::milliseconds(10));
+    }
+
+    EXPECT_EQ(connection.getState(), State::Connecting);
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] MeshManager compiles and runs
+- [ ] HTTP endpoints return correct data
+- [ ] All integration tests pass
+- [ ] No deadlocks in manual testing
+
+---
+
+### Phase B.5: Performance Benchmarking
+
+**Objective**: Verify performance improvements match design estimates.
+
+**Duration**: 1-2 hours
+
+#### Tasks
+
+1. **Create benchmark suite**
+   - New file: `tests/performance/NetworkConnectionBenchmark.cpp`
+
+```cpp
+#include <benchmark/benchmark.h>
+
+static void BM_GetState(benchmark::State& state) {
+    NetworkConnection conn(testNode);
+
+    for (auto _ : state) {
+        auto s = conn.getState();  // Should be ~10ns
+        benchmark::DoNotOptimize(s);
+    }
+}
+BENCHMARK(BM_GetState);
+
+static void BM_GetDevices(benchmark::State& state) {
+    NetworkConnection conn(testNode);
+    conn.connect();
+
+    for (auto _ : state) {
+        auto devices = conn.getRemoteDevices();  // Should be <1ms
+        benchmark::DoNotOptimize(devices);
+    }
+}
+BENCHMARK(BM_GetDevices);
+
+static void BM_ConcurrentQueries(benchmark::State& state) {
+    NetworkConnection conn(testNode);
+
+    std::vector<std::thread> threads;
+    for (int i = 0; i < state.range(0); ++i) {
+        threads.emplace_back([&conn] {
+            for (int j = 0; j < 1000; ++j) {
+                conn.getState();
+            }
+        });
+    }
+
+    for (auto& t : threads) {
+        t.join();
+    }
+}
+BENCHMARK(BM_ConcurrentQueries)->Range(1, 128);
+```
+
+2. **Run benchmarks**
+
+```bash
+cmake --build build --target NetworkConnectionBenchmark
+./build/tests/performance/NetworkConnectionBenchmark --benchmark_format=json > results.json
+```
+
+3. **Compare metrics**
+
+| Operation | Current | Target | Actual | Pass/Fail |
+|-----------|---------|--------|--------|-----------|
+| getState() | 100ns | <20ns | ??? | ??? |
+| getRemoteDevices() | 100ns | <1ms | ??? | ??? |
+| 100 threads querying | 10ms | <1ms | ??? | ??? |
+
+#### Acceptance Criteria
+
+- [ ] getState() <20ns (atomic read)
+- [ ] getRemoteDevices() <1ms (command latency)
+- [ ] 100 concurrent threads <1ms total
+- [ ] No deadlocks in stress tests
+
+---
+
+### Phase B.6: TSAN & Stress Testing
+
+**Objective**: Prove zero race conditions and deadlocks.
+
+**Duration**: 1-2 hours
+
+#### Tasks
+
+1. **Build with ThreadSanitizer**
+
+```bash
+cmake -B build-tsan -DCMAKE_CXX_FLAGS="-fsanitize=thread -g -O1" -DCMAKE_EXE_LINKER_FLAGS="-fsanitize=thread"
+cmake --build build-tsan
+```
+
+2. **Run stress tests**
+
+```bash
+./build-tsan/tests/unit/NetworkConnectionTest --gtest_filter="*Stress*"
+```
+
+3. **Create dedicated stress test**
+
+```cpp
+TEST(NetworkConnectionSEDA, StressTest_100Threads_60Seconds) {
+    NetworkConnection conn(testNode);
+    conn.connect();
+
+    std::atomic<bool> stop{false};
+    std::atomic<int> totalQueries{0};
+
+    std::vector<std::thread> readers;
+    for (int i = 0; i < 100; ++i) {
+        readers.emplace_back([&] {
+            while (!stop) {
+                conn.getState();
+                conn.getRemoteNode();
+                conn.getRemoteDevices();
+                ++totalQueries;
+            }
+        });
+    }
+
+    std::this_thread::sleep_for(std::chrono::seconds(60));
+    stop = true;
+
+    for (auto& t : readers) {
+        t.join();
+    }
+
+    std::cout << "Total queries: " << totalQueries << std::endl;
+    // Should be >1M queries in 60 seconds
+    EXPECT_GT(totalQueries, 1000000);
+}
+```
+
+#### Acceptance Criteria
+
+- [ ] ThreadSanitizer reports zero warnings
+- [ ] Stress test runs for 60 seconds without crash
+- [ ] >1M queries/minute throughput
+- [ ] Memory usage stable (no leaks)
+
+---
+
+## Decision Criteria
+
+### When to Use Each Approach
+
+#### Choose Approach A (Mutex Fixes) if:
+
+- ✅ No confirmed deadlocks in production
+- ✅ Mesh size expected <20 connections
+- ✅ Query latency <1ms acceptable
+- ✅ Development time limited
+- ✅ Lower risk tolerance
+
+#### Choose Approach B (Full SEDA) if:
+
+- ✅ Deadlocks occurring in production
+- ✅ Metrics show >5% mutex contention
+- ✅ Mesh size expected >50 connections
+- ✅ Query latency <100µs required
+- ✅ Long-term scalability critical
+
+#### Metrics to Collect After Approach A
+
+```cpp
+// Add instrumentation to NetworkConnection
+class NetworkConnection {
+private:
+    // Contention metrics
+    std::atomic<uint64_t> mutexContentionCount{0};
+    std::atomic<uint64_t> totalMutexAcquisitions{0};
+
+    State getState() const {
+        totalMutexAcquisitions.fetch_add(1);
+
+        std::unique_lock<std::timed_mutex> lock(stateMutex, std::chrono::milliseconds(10));
+        if (!lock.owns_lock()) {
+            mutexContentionCount.fetch_add(1);
+            // ... fallback ...
+        }
+        // ...
+    }
+
+public:
+    double getContentionPercentage() const {
+        uint64_t total = totalMutexAcquisitions.load();
+        return total > 0 ? (100.0 * mutexContentionCount / total) : 0.0;
+    }
+};
+```
+
+**Decision Point**: If contention >5% after 1 week of monitoring → Proceed to Approach B.
+
+---
+
+## Integration with Current Phase 6
+
+### Phase 6 Context
+
+According to the project workplan, Phase 6 focuses on:
+- Network mesh topology management
+- Heartbeat monitoring for connection health
+- HTTP endpoints for mesh queries
+
+### Integration Points
+
+#### 1. MeshManager Integration
+
+**Current State**: MeshManager uses ConnectionPool to manage NetworkConnection instances.
+
+**Approach A Impact**:
+- No API changes needed
+- MeshManager continues to call `getState()`, `getRemoteDevices()` as before
+- Callbacks dispatch via message thread (may need test updates)
+
+**Approach B Impact**:
+- No API changes needed (same public interface)
+- Query performance improves significantly
+- May need to update tests for async behavior
+
+#### 2. HeartbeatMonitor Integration
+
+**Current State**: HeartbeatMonitor timer calls `getTimeSinceLastHeartbeat()` every 1 second.
+
+**Approach A Impact**:
+- Minor: Callback dispatch becomes async
+- Test update needed to poll for state changes
+
+**Approach B Impact**:
+- Significant performance improvement (atomic read vs mutex)
+- Must post CheckHeartbeatCommand instead of direct setState()
+
+**Required Change**:
+
+```cpp
+// HeartbeatMonitor.cpp
+void HeartbeatMonitor::checkTimeouts()
+{
+    for (auto* connection : connections) {
+        if (connection->getTimeSinceLastHeartbeat() > timeoutMs) {
+            // APPROACH A: Direct call works (async callback)
+            connection->setState(State::Failed);
+
+            // APPROACH B: Post command
+            connection->checkHeartbeat();  // Posts CheckHeartbeatCommand
+        }
+    }
+}
+```
+
+#### 3. HTTP Endpoint Integration
+
+**Current State**: HTTP handlers call `MeshManager::getStatistics()` which queries all connections.
+
+**Approach A Impact**:
+- None - queries work identically
+- Potential latency improvement from reduced contention
+
+**Approach B Impact**:
+- Complex queries (getRemoteDevices) may add <1ms latency
+- Simple queries (getState) become 100x faster
+
+**No changes needed** - public API remains identical.
+
+---
+
+## Testing & Validation Strategy
+
+### Test Matrix
+
+| Test Category | Approach A | Approach B | Purpose |
+|--------------|------------|------------|---------|
+| Unit Tests | Update callbacks | Update callbacks + async | Verify correctness |
+| Integration Tests | Minor updates | Moderate updates | Verify subsystem interaction |
+| Concurrency Tests | New: 100 threads | New: stress test | Verify no deadlocks |
+| Performance Tests | Baseline only | Full benchmarks | Verify improvements |
+| Regression Tests | Full suite | Full suite | Verify no breakage |
+| TSAN Tests | Required | Required | Verify no races |
+
+### Validation Plan
+
+#### Pre-Implementation Baseline
+
+1. **Capture current metrics**
+
+```bash
+# Build baseline
+cmake -B build-baseline
+cmake --build build-baseline
+
+# Run baseline tests
+./build-baseline/tests/unit/NetworkConnectionTest
+
+# Run baseline benchmarks
+./build-baseline/tests/performance/NetworkConnectionBenchmark > baseline.json
+```
+
+2. **Create test checklist**
+
+```markdown
+- [ ] All 47 existing unit tests pass
+- [ ] Integration test suite passes (mesh topology)
+- [ ] Manual test: Connect 5 nodes, query mesh 100 times
+- [ ] No crashes in 5-minute stress test
+- [ ] ThreadSanitizer clean
+```
+
+#### Post-Implementation Validation
+
+1. **Regression testing**
+
+```bash
+# Build with fixes
+cmake -B build-fixed
+cmake --build build-fixed
+
+# Run all tests
+ctest --test-dir build-fixed --output-on-failure
+
+# Compare results
+diff baseline-results.txt fixed-results.txt
+```
+
+2. **Performance validation**
+
+```bash
+# Run benchmarks
+./build-fixed/tests/performance/NetworkConnectionBenchmark > fixed.json
+
+# Compare
+python3 scripts/compare_benchmarks.py baseline.json fixed.json
+```
+
+Expected output:
+```
+Metric              | Baseline | Fixed   | Change
+--------------------|----------|---------|--------
+getState() latency  | 120ns    | 115ns   | -4.2%  ✅
+getDevices() latency| 150ns    | 145ns   | -3.3%  ✅
+Contention rate     | 0.8%     | 0.1%    | -87.5% ✅
+```
+
+3. **Deadlock validation**
+
+```bash
+# ThreadSanitizer build
+cmake -B build-tsan -DCMAKE_CXX_FLAGS="-fsanitize=thread"
+cmake --build build-tsan
+
+# Run concurrency tests
+./build-tsan/tests/unit/NetworkConnectionTest --gtest_filter="*Concurrent*"
+
+# Expected: No TSAN warnings
+```
+
+### Load Testing Strategy
+
+#### Scenario 1: Single Node Under Load
+
+```cpp
+TEST(NetworkConnection, LoadTest_SingleNode_1000QPS) {
+    NetworkConnection conn(testNode);
+    conn.connect();
+
+    auto start = std::chrono::steady_clock::now();
+    int queries = 0;
+
+    while (queries < 10000) {
+        conn.getState();
+        conn.getRemoteDevices();
+        ++queries;
+    }
+
+    auto elapsed = std::chrono::steady_clock::now() - start;
+    auto qps = queries / std::chrono::duration<double>(elapsed).count();
+
+    EXPECT_GT(qps, 1000);  // Should achieve >1000 QPS
+}
+```
+
+#### Scenario 2: Mesh Under Load
+
+```cpp
+TEST(MeshManager, LoadTest_10Nodes_100QPS_Each) {
+    MeshManager mesh;
+
+    // Add 10 nodes
+    for (int i = 0; i < 10; ++i) {
+        mesh.addPeer(createTestNode(i));
+    }
+
+    // Query all nodes 1000 times (100 QPS × 10 nodes)
+    auto start = std::chrono::steady_clock::now();
+
+    for (int i = 0; i < 1000; ++i) {
+        auto stats = mesh.getStatistics();  // Queries all 10 nodes
+        EXPECT_EQ(stats.totalConnections, 10);
+    }
+
+    auto elapsed = std::chrono::steady_clock::now() - start;
+
+    // Should complete in <10 seconds (1000 queries / 100 QPS)
+    EXPECT_LT(elapsed, std::chrono::seconds(10));
+}
+```
+
+#### Scenario 3: Chaos Testing
+
+```cpp
+TEST(NetworkConnection, ChaosTest_RandomOperations) {
+    NetworkConnection conn(testNode);
+
+    std::random_device rd;
+    std::mt19937 gen(rd());
+    std::uniform_int_distribution<> dis(0, 5);
+
+    for (int i = 0; i < 10000; ++i) {
+        switch (dis(gen)) {
+            case 0: conn.connect(); break;
+            case 1: conn.disconnect(); break;
+            case 2: conn.getState(); break;
+            case 3: conn.getRemoteNode(); break;
+            case 4: conn.getRemoteDevices(); break;
+            case 5: conn.checkHeartbeat(); break;
+        }
+
+        // Random delay 0-10ms
+        std::this_thread::sleep_for(
+            std::chrono::milliseconds(dis(gen) * 2));
+    }
+
+    // Should survive chaos without crash
+}
+```
+
+---
+
+## Timeline & Resources
+
+### Approach A: Mutex Fixes Timeline
+
+| Phase | Duration | Developer Hours | Dependencies |
+|-------|----------|----------------|--------------|
+| A.1: Fix callback-under-lock | 30-45 min | 0.75h | None |
+| A.2: Protect remoteDevices | 45-60 min | 1.0h | None |
+| A.3: Add mutex timeouts | 30-45 min | 0.75h | None |
+| A.4: Fix MeshManager lifecycle | 45-60 min | 1.0h | None |
+| A.5: Validation & testing | 30-45 min | 0.75h | All previous |
+| **Total** | **3-4 hours** | **4.25h** | - |
+
+**Critical Path**: Linear (each phase builds on previous)
+**Parallelization**: Minimal (single developer)
+**Calendar Time**: 1 day (with buffer for testing)
+
+### Approach B: Full SEDA Timeline
+
+| Phase | Duration | Developer Hours | Dependencies |
+|-------|----------|----------------|--------------|
+| B.1: Infrastructure setup | 2-3 hours | 2.5h | None |
+| B.2: State migration | 2-3 hours | 2.5h | B.1 |
+| B.3: Mutex removal | 1-2 hours | 1.5h | B.2 |
+| B.4: Integration updates | 2-3 hours | 2.5h | B.3 |
+| B.5: Performance benchmarking | 1-2 hours | 1.5h | B.4 |
+| B.6: TSAN & stress testing | 1-2 hours | 1.5h | B.4 |
+| **Total** | **9-15 hours** | **12h** | - |
+
+**Critical Path**: B.1 → B.2 → B.3 → B.4 → (B.5 + B.6 parallel)
+**Parallelization**: B.5 and B.6 can run in parallel
+**Calendar Time**: 2-3 days (with buffer for debugging)
+
+### Resource Requirements
+
+#### Approach A
+
+- **Developers**: 1 senior C++ developer
+- **Skills Required**:
+  - C++17 threading (std::mutex, std::atomic)
+  - JUCE framework (MessageManager)
+  - Unit testing (GTest)
+- **Infrastructure**:
+  - Development machine with TSAN support
+  - CI/CD pipeline with sanitizer builds
+
+#### Approach B
+
+- **Developers**: 1 senior C++ developer + 1 reviewer
+- **Skills Required**:
+  - Advanced C++ concurrency patterns
+  - JUCE threading model (juce::Thread, juce::WaitableEvent)
+  - Event-driven architecture design
+  - Performance profiling tools
+- **Infrastructure**:
+  - Development machine with TSAN support
+  - Benchmarking environment (isolated CPU cores)
+  - CI/CD with performance regression detection
+
+### External Dependencies
+
+- **JUCE Framework**: Version 6.0+ (for modern threading primitives)
+- **httplib**: Version 0.14.3 (already integrated)
+- **GTest**: For unit testing
+- **Google Benchmark**: For performance testing (optional for Approach A, required for B)
+- **ThreadSanitizer**: Part of Clang/GCC toolchain
+
+---
+
+## Risk Assessment
+
+### Approach A Risks
+
+| Risk | Probability | Impact | Mitigation |
+|------|------------|--------|------------|
+| Callback timing breaks tests | Medium | Low | Update tests to poll for callbacks |
+| Atomic shared_ptr not available in C++17 | Low | Medium | Use C++20 or mutex fallback |
+| Mutex timeouts cause false alarms | Low | Low | Tune timeout values (100-500ms) |
+| Contention persists after fixes | Medium | Medium | Be prepared to escalate to Approach B |
+| Performance regression | Low | Low | Run baseline benchmarks first |
+
+**Overall Risk**: Low
+
+### Approach B Risks
+
+| Risk | Probability | Impact | Mitigation |
+|------|------------|--------|------------|
+| Command latency breaks real-time requirements | Low | High | Use atomic snapshots for hot paths |
+| Worker thread doesn't shut down cleanly | Medium | Medium | Implement robust shutdown with timeout |
+| Query commands deadlock | Low | Critical | Use WaitableEvent with timeout |
+| Increased memory usage (command queue) | Low | Low | Monitor queue depth, add bounds |
+| Complexity increases bug surface area | Medium | Medium | Comprehensive testing, code review |
+| Migration breaks existing tests | High | Medium | Update tests incrementally, maintain backward compat |
+| Performance doesn't meet targets | Low | High | Benchmark early (Phase B.1), pivot if needed |
+
+**Overall Risk**: Medium
+
+### Mitigation Strategies
+
+#### For Both Approaches
+
+1. **Feature Branch Development**
+   - Create `feature/mutex-fixes` or `feature/seda-migration` branch
+   - No direct commits to main
+   - Require PR review before merge
+
+2. **Incremental Testing**
+   - Run tests after each phase
+   - Use CI/CD to catch regressions early
+   - Maintain passing tests at all times
+
+3. **Performance Monitoring**
+   - Establish baseline metrics before changes
+   - Monitor metrics after each phase
+   - Rollback if regressions >10%
+
+4. **Canary Deployment**
+   - Deploy to test environment first
+   - Monitor for 24-48 hours
+   - Gradual rollout to production
+
+---
+
+## Rollback Procedures
+
+### Approach A Rollback
+
+**Simplicity**: Trivial (single commit revert)
+
+```bash
+# If issues detected after merge
+git revert <commit-hash>
+git push origin main
+
+# Rebuild
+cmake --build build
+ctest --test-dir build
+```
+
+**Recovery Time**: <10 minutes
+
+**Data Loss**: None (no state migration)
+
+### Approach B Rollback
+
+**Complexity**: Moderate (may span multiple commits)
+
+#### Option 1: Clean Revert (If Single Merge Commit)
+
+```bash
+git revert -m 1 <merge-commit>
+git push origin main
+```
+
+#### Option 2: Branch Revert (If Multiple Commits)
+
+```bash
+# Create rollback branch
+git checkout -b rollback/seda-migration
+
+# Revert range of commits
+git revert <first-commit>..<last-commit>
+
+# Test rollback
+cmake --build build
+ctest --test-dir build
+
+# Push rollback
+git push origin rollback/seda-migration
+```
+
+#### Option 3: Forward Fix (If Partial Issues)
+
+If only specific components broken:
+
+```bash
+# Fix individual issues without full rollback
+git checkout main
+git cherry-pick <working-commit>
+```
+
+**Recovery Time**: 30-60 minutes
+
+**Data Loss**: None (runtime state only)
+
+### Rollback Decision Criteria
+
+| Severity | Symptom | Action | Timeline |
+|----------|---------|--------|----------|
+| P0 | Production deadlocks | Immediate rollback | <15 min |
+| P1 | Crashes in <1% of requests | Rollback within 1 hour | <1 hour |
+| P2 | Performance regression >20% | Rollback within 4 hours | <4 hours |
+| P3 | Minor test failures | Forward fix | <1 day |
+
+### Rollback Testing
+
+Before deploying either approach, verify rollback procedure:
+
+```bash
+# Simulate rollback in test environment
+git checkout test-environment
+git merge feature/mutex-fixes
+# ... deploy, verify working ...
+git revert HEAD
+# ... verify rollback successful ...
+```
+
+---
+
+## Appendix A: Command Reference
+
+### Build Commands
+
+```bash
+# Clean build
+rm -rf build && cmake -B build && cmake --build build
+
+# Build with sanitizers
+cmake -B build-tsan -DCMAKE_CXX_FLAGS="-fsanitize=thread -g"
+cmake --build build-tsan
+
+# Build with optimizations
+cmake -B build-release -DCMAKE_BUILD_TYPE=Release
+cmake --build build-release
+
+# Run tests
+ctest --test-dir build --output-on-failure
+
+# Run specific test
+./build/tests/unit/NetworkConnectionTest --gtest_filter="*Concurrent*"
+```
+
+### Monitoring Commands
+
+```bash
+# Check for deadlocks (requires gdb)
+gdb -batch -ex "thread apply all bt" ./build/MidiHttpServer
+
+# Monitor queue depth (add to ConnectionWorker)
+watch -n 1 "grep 'Queue depth' /var/log/midi-server.log"
+
+# Check contention percentage
+curl http://localhost:8080/debug/metrics | jq '.contention_percent'
+```
+
+---
+
+## Appendix B: Checklist Summary
+
+### Approach A Checklist
+
+- [ ] Phase A.1: Callback-under-lock fixed
+  - [ ] setState() releases locks before callback
+  - [ ] Callbacks dispatched via MessageManager
+  - [ ] Tests updated for async callbacks
+- [ ] Phase A.2: remoteDevices protected
+  - [ ] Mutex or atomic shared_ptr protecting reads/writes
+  - [ ] No TSAN warnings
+- [ ] Phase A.3: Mutex timeouts added
+  - [ ] All mutexes use std::timed_mutex
+  - [ ] Timeouts logged
+  - [ ] Fallback to atomics where possible
+- [ ] Phase A.4: MeshManager fixed
+  - [ ] Pool iteration protected from use-after-free
+  - [ ] withAllConnections helper implemented
+- [ ] Phase A.5: Validation complete
+  - [ ] All tests pass
+  - [ ] Benchmarks show no regression
+  - [ ] TSAN clean
+  - [ ] Contention metrics <1%
+
+### Approach B Checklist
+
+- [ ] Phase B.1: Infrastructure
+  - [ ] NetworkCommands.h created
+  - [ ] CommandQueue.h created and tested
+  - [ ] ConnectionWorker.h created
+  - [ ] Worker thread lifecycle tested
+- [ ] Phase B.2: State migration
+  - [ ] NetworkConnection uses worker
+  - [ ] Atomic snapshots for getState(), getHeartbeat()
+  - [ ] Query commands for complex data
+  - [ ] All handlers implemented
+- [ ] Phase B.3: Mutex removal
+  - [ ] All mutex fields removed
+  - [ ] All lock_guard removed
+  - [ ] TSAN clean
+- [ ] Phase B.4: Integration
+  - [ ] MeshManager works with new API
+  - [ ] HeartbeatMonitor works with new API
+  - [ ] HTTP handlers work with new API
+  - [ ] All integration tests pass
+- [ ] Phase B.5: Performance
+  - [ ] Benchmarks created
+  - [ ] getState() <20ns
+  - [ ] getRemoteDevices() <1ms
+  - [ ] 100 threads <1ms total
+- [ ] Phase B.6: Stress testing
+  - [ ] TSAN build passes
+  - [ ] 60-second stress test passes
+  - [ ] >1M queries/minute achieved
+  - [ ] No memory leaks
+
+---
+
+## Appendix C: Metrics Dashboard
+
+### Recommended Metrics to Track
+
+```cpp
+// Add to NetworkConnection (both approaches)
+struct ConnectionMetrics {
+    std::atomic<uint64_t> totalQueries{0};
+    std::atomic<uint64_t> mutexContentions{0};
+    std::atomic<uint64_t> callbacksDispatched{0};
+    std::atomic<uint64_t> commandsProcessed{0};  // SEDA only
+
+    // Latency histogram (requires external library)
+    // histogram<int64_t> queryLatencyNs;
+
+    void recordQuery(int64_t latencyNs) {
+        totalQueries.fetch_add(1);
+        // queryLatencyNs.record(latencyNs);
+    }
+
+    nlohmann::json toJson() const {
+        return {
+            {"total_queries", totalQueries.load()},
+            {"mutex_contentions", mutexContentions.load()},
+            {"contention_percent", getContentionPercent()},
+            {"callbacks_dispatched", callbacksDispatched.load()},
+            {"commands_processed", commandsProcessed.load()}
+        };
+    }
+
+    double getContentionPercent() const {
+        uint64_t total = totalQueries.load();
+        return total > 0 ? (100.0 * mutexContentions / total) : 0.0;
+    }
+};
+```
+
+### HTTP Debug Endpoint
+
+```cpp
+// Add to HTTP server
+svr.Get("/debug/connection/:uuid/metrics", [](const auto& req, auto& res) {
+    auto uuid = req.path_params.at("uuid");
+    auto* conn = meshManager.getConnection(uuid);
+
+    if (!conn) {
+        res.status = 404;
+        return;
+    }
+
+    res.set_content(conn->getMetrics().toJson().dump(), "application/json");
+});
+```
+
+---
+
+**Document Status**: Ready for Execution
+**Recommended Start**: Approach A (Mutex Fixes)
+**Decision Point**: Re-evaluate after 1 week of production monitoring
+**Next Steps**: Create feature branch and begin Phase A.1
diff --git a/modules/juce/midi-server/docs/1.0/seda/planning/design.md b/modules/juce/midi-server/docs/1.0/seda/planning/design.md
new file mode 100644
index 0000000..d3c7e2e
--- /dev/null
+++ b/modules/juce/midi-server/docs/1.0/seda/planning/design.md
@@ -0,0 +1,1145 @@
+# SEDA Architecture Design for Network MIDI Server
+
+**Document Version**: 1.0
+**Date**: 2025-10-06
+**Status**: Planning
+**Authors**: Claude Code Architecture Team
+
+---
+
+## Table of Contents
+
+1. [Executive Summary](#executive-summary)
+2. [Current Architecture Analysis](#current-architecture-analysis)
+3. [SEDA Overview](#seda-overview)
+4. [Proposed SEDA Architecture](#proposed-seda-architecture)
+5. [JUCE Implementation Patterns](#juce-implementation-patterns)
+6. [Stage Decomposition](#stage-decomposition)
+7. [Command/Event Design](#commandevent-design)
+8. [Performance Considerations](#performance-considerations)
+9. [Migration Strategy](#migration-strategy)
+10. [Testing Strategy](#testing-strategy)
+11. [Architecture Review](#architecture-review)
+12. [Decision Matrix](#decision-matrix)
+13. [Appendices](#appendices)
+
+---
+
+## Executive Summary
+
+The current `NetworkConnection` implementation suffers from deadlock potential due to mutex-based synchronization across
+multiple threads (HTTP handshake thread, heartbeat monitor timer thread, and HTTP query threads). This document proposes
+a Staged Event-Driven Architecture (SEDA) solution using JUCE primitives to eliminate lock contention.
+
+### Key Findings
+
+- **Current Issue**: Three mutexes (`stateMutex`, `heartbeatMutex`, `messageMutex`) create potential for
+  callback-under-lock deadlocks
+- **Root Cause**: HTTP query threads blocking on mutexes held by handshake/heartbeat threads
+- **Proposed Solution**: SEDA with single worker thread per stage, eliminating shared mutex contention
+- **Expected Benefits**: Zero deadlocks, 10x query performance, easier testing
+- **Implementation Effort**: 8-14 hours for complete migration
+
+---
+
+## Current Architecture Analysis
+
+### Thread Model
+
+The system currently involves **4+ concurrent thread contexts**:
+
+1. **HTTP Handshake Thread** - Spawned by `juce::Thread::launch()` in `connect()` (NetworkConnection.cpp:53-55)
+2. **JUCE Message Thread** - Runs `HeartbeatMonitor::timerCallback()` every 1 second
+3. **HTTP Server Threads** - Multiple threads serving HTTP GET requests (e.g., `/network/mesh`)
+4. **UDP Receiver Thread** - (Future) Will handle incoming UDP packets
+
+### Current Mutex Usage
+
+#### `stateMutex` (NetworkConnection.h:245)
+
+**Protected Resources:**
+
+- `currentState` (atomic, but mutex still used)
+- `remoteNodeInfo` (read in `getRemoteNode()`, written during construction)
+- Connection lifecycle operations
+
+**Critical Sections:**
+
+```cpp
+// NetworkConnection.cpp:42-56 - connect()
+std::lock_guard<std::mutex> lock(stateMutex);
+// Checks state, sets Connecting, spawns handshake thread
+
+// NetworkConnection.cpp:61-75 - disconnect()
+std::lock_guard<std::mutex> lock(stateMutex);
+// Shuts down sockets, resets state
+
+// NetworkConnection.cpp:86-88 - getRemoteNode()
+std::lock_guard<std::mutex> lock(stateMutex);
+return remoteNodeInfo;
+
+// NetworkConnection.cpp:229-231 - performHandshake() success path
+std::lock_guard<std::mutex> lock(stateMutex);
+setState(State::Connected);
+
+// NetworkConnection.cpp:245-246 - performHandshake() failure path
+std::lock_guard<std::mutex> lock(stateMutex);
+setState(State::Failed);
+
+// NetworkConnection.cpp:150-156 - checkHeartbeat()
+std::lock_guard<std::mutex> lock(stateMutex);
+setState(State::Failed);
+```
+
+**Deadlock Scenario:**
+
+```
+Thread A (HTTP Query):          Thread B (Handshake):
+getStatistics()                 performHandshake()
+  -> getTotalDeviceCount()        [Holds NO lock initially]
+    -> getRemoteDevices()         Completing handshake...
+      -> getState()               Acquires stateMutex (line 229)
+        [Blocked waiting]         setState(Connected)
+                                  onStateChanged callback()
+                                    -> MeshManager handler
+                                      -> getRemoteNode()
+                                        [Tries to acquire stateMutex]
+                                        [DEADLOCK if callback is sync]
+```
+
+#### `heartbeatMutex` (NetworkConnection.h:248)
+
+**Protected Resources:**
+
+- `lastHeartbeatTime` (juce::int64)
+
+**Critical Sections:**
+
+```cpp
+// NetworkConnection.cpp:129-131 - getTimeSinceLastHeartbeat()
+std::lock_guard<std::mutex> lock(heartbeatMutex);
+return juce::Time::getCurrentTime().toMilliseconds() - lastHeartbeatTime;
+
+// NetworkConnection.cpp:234-237 - performHandshake() success
+std::lock_guard<std::mutex> lock(heartbeatMutex);
+lastHeartbeatTime = juce::Time::getCurrentTime().toMilliseconds();
+
+// NetworkConnection.cpp:303-305 - notifyHeartbeatReceived()
+std::lock_guard<std::mutex> lock(heartbeatMutex);
+lastHeartbeatTime = juce::Time::getCurrentTime().toMilliseconds();
+```
+
+**Contention Points:**
+
+- `HeartbeatMonitor::checkTimeouts()` calls `getTimeSinceLastHeartbeat()` on timer thread (every 1s)
+- HTTP threads calling `getStatistics()` -> `isAlive()` -> `getTimeSinceLastHeartbeat()`
+- UDP receiver thread calling `notifyHeartbeatReceived()`
+
+#### `messageMutex` (NetworkConnection.h:243)
+
+**Protected Resources:**
+
+- `receivedMessages` (std::vector<MidiMessage>)
+
+**Critical Sections:**
+
+```cpp
+// NetworkConnection.cpp:120-124 - getReceivedMessages()
+std::lock_guard<std::mutex> lock(messageMutex);
+std::vector<MidiMessage> result = std::move(receivedMessages);
+receivedMessages.clear();
+return result;
+```
+
+**Future Contention:**
+
+- UDP receiver thread appending to `receivedMessages`
+- MIDI consumer thread calling `getReceivedMessages()`
+
+### Race Conditions Identified
+
+#### Race #1: State Query During State Transition
+
+```
+Thread A (Query):                Thread B (Handshake):
+getState() -> returns Connecting setState(Connected)
+                                 onStateChanged() callback fires
+[Uses stale state]               [State already changed]
+```
+
+**Impact:** Low severity - atomic reads ensure valid state value
+
+#### Race #2: Callback Invocation Under Lock
+
+```cpp
+// NetworkConnection.cpp:255-271 - setState()
+void NetworkConnection::setState(State newState)
+{
+    State oldState = currentState.load();
+    // ...
+    currentState = newState;
+    // ...
+    if (onStateChanged) {
+        onStateChanged(oldState, newState);  // CALLED WITHOUT LOCK!
+    }
+}
+```
+
+**Impact:** Medium severity - Callback might query state while caller still holds lock elsewhere
+
+#### Race #3: Device List Read Without Protection
+
+```cpp
+// NetworkConnection.cpp:91-96 - getRemoteDevices()
+std::vector<DeviceInfo> NetworkConnection::getRemoteDevices() const
+{
+    // No mutex needed: remoteDevices is only written once during handshake
+    // and this is a read-only copy operation
+    return remoteDevices;  // UNSAFE if handshake ongoing
+}
+```
+
+**Impact:** High severity - Comment claims safety, but `performHandshake()` writes to `remoteDevices` (lines 213-223)
+concurrently with reads
+
+#### Race #4: Multiple Pool Operations
+
+```cpp
+// MeshManager.cpp:147-166 - getTotalDeviceCount()
+auto connections = connectionPool.getAllConnections();  // Acquires pool mutex
+for (auto* connection : connections) {                  // Releases pool mutex
+    if (connection && connection->getState() == NetworkConnection::State::Connected) {
+        total += (int)connection->getRemoteDevices().size();  // Connection may be deleted!
+    }
+}
+```
+
+**Impact:** Critical severity - Connection may be removed from pool between `getAllConnections()` and dereferencing
+
+---
+
+## SEDA Overview
+
+### SEDA Principles
+
+Staged Event-Driven Architecture (SEDA) decomposes complex event-driven applications into:
+
+1. **Stages** - Self-contained processing units with private state
+2. **Event Queues** - Bounded queues connecting stages
+3. **Stage Workers** - Thread pools servicing each stage
+4. **Controllers** - Dynamic resource allocation (optional)
+
+**Key Benefits:**
+
+- Eliminates shared mutable state between threads
+- No mutex contention - each stage owns its data
+- Natural backpressure via bounded queues
+- Easily testable and observable
+
+### SEDA for NetworkConnection
+
+```
+┌─────────────────────────────────────────────────────────────┐
+│                      NetworkConnection                       │
+├─────────────────────────────────────────────────────────────┤
+│                                                               │
+│  ┌──────────────┐      ┌──────────────┐      ┌───────────┐ │
+│  │ HTTP Thread  │      │ Timer Thread │      │  UDP Recv │ │
+│  │  (External)  │      │  (External)  │      │ (External)│ │
+│  └──────┬───────┘      └──────┬───────┘      └─────┬─────┘ │
+│         │                     │                     │        │
+│         v                     v                     v        │
+│  ┌──────────────────────────────────────────────────────┐  │
+│  │            Command Queue (Lock-Free FIFO)             │  │
+│  │  - ConnectCmd                                         │  │
+│  │  - DisconnectCmd                                      │  │
+│  │  - CheckHeartbeatCmd                                  │  │
+│  │  - NotifyHeartbeatCmd                                 │  │
+│  │  - SendMidiCmd                                        │  │
+│  │  - GetStateQueryCmd (with response mechanism)        │  │
+│  └──────────────────┬───────────────────────────────────┘  │
+│                     v                                        │
+│  ┌──────────────────────────────────────────────────────┐  │
+│  │           Connection State Machine Worker            │  │
+│  │           (Single juce::Thread)                       │  │
+│  │                                                        │  │
+│  │  Private State:                                       │  │
+│  │  - currentState                                       │  │
+│  │  - remoteNodeInfo                                     │  │
+│  │  - remoteDevices                                      │  │
+│  │  - lastHeartbeatTime                                  │  │
+│  │  - udpSocket, httpClient                             │  │
+│  │  - receivedMessages                                   │  │
+│  │                                                        │  │
+│  │  NO MUTEXES - Single-threaded access                 │  │
+│  └──────────────────┬───────────────────────────────────┘  │
+│                     │                                        │
+│                     v                                        │
+│  ┌──────────────────────────────────────────────────────┐  │
+│  │          Notification/Callback Queue                  │  │
+│  │          (Dispatched to message thread)               │  │
+│  │  - onStateChanged()                                   │  │
+│  │  - onError()                                          │  │
+│  │  - onDevicesReceived()                                │  │
+│  └──────────────────────────────────────────────────────┘  │
+│                                                               │
+└─────────────────────────────────────────────────────────────┘
+```
+
+---
+
+## Proposed SEDA Architecture
+
+### Simplified Two-Stage Architecture
+
+```
+┌──────────────────────────────────────────────────────────┐
+│                   Main Stage Worker                       │
+│            (Single juce::Thread - Event Loop)             │
+│                                                            │
+│  Handles:                                                 │
+│  - Connection lifecycle (connect/disconnect)              │
+│  - Heartbeat tracking                                     │
+│  - State queries (via command)                           │
+│  - MIDI message queuing                                  │
+│                                                            │
+│  State: ALL connection state (no mutexes)                │
+└──────────┬───────────────────────────────────────────────┘
+           │
+           │ Output Events
+           v
+┌──────────────────────────────────────────────────────────┐
+│              Notification Stage (Message Thread)          │
+│                                                            │
+│  Dispatches callbacks:                                    │
+│  - onStateChanged()                                       │
+│  - onError()                                              │
+│  - onDevicesReceived()                                    │
+│  - onMidiMessageReceived()                                │
+└──────────────────────────────────────────────────────────┘
+```
+
+---
+
+## JUCE Implementation Patterns
+
+### Event Queue Implementation Options
+
+#### Option A: `juce::AbstractFifo` + Ring Buffer (Lock-Free)
+
+**Best for:** High-frequency events (MIDI messages, heartbeats)
+
+```cpp
+// Lock-free single-producer, single-consumer queue
+class CommandQueue {
+    static constexpr int QUEUE_SIZE = 1024;
+
+    struct Command {
+        enum Type { Connect, Disconnect, CheckHeartbeat, SendMidi, GetState };
+        Type type;
+        // ... variant payload
+    };
+
+    juce::AbstractFifo fifo{QUEUE_SIZE};
+    Command buffer[QUEUE_SIZE];
+
+public:
+    bool push(const Command& cmd) {
+        int start1, size1, start2, size2;
+        fifo.prepareToWrite(1, start1, size1, start2, size2);
+        if (size1 == 0) return false;  // Queue full
+
+        buffer[start1] = cmd;
+        fifo.finishedWrite(1);
+        return true;
+    }
+
+    bool pop(Command& cmd) {
+        int start1, size1, start2, size2;
+        fifo.prepareToRead(1, start1, size1, start2, size2);
+        if (size1 == 0) return false;  // Queue empty
+
+        cmd = buffer[start1];
+        fifo.finishedRead(1);
+        return true;
+    }
+};
+```
+
+**Pros:**
+
+- True lock-free (wait-free for single producer/consumer)
+- Constant-time operations
+- Cache-friendly
+
+**Cons:**
+
+- Fixed capacity (bounded queue)
+- Complex for multi-producer/multi-consumer
+- Requires careful memory ordering
+
+#### Option B: `juce::WaitableEvent` + `std::vector` with Mutex
+
+**Best for:** Low-frequency events (state changes, queries)
+
+```cpp
+class CommandQueue {
+    std::mutex mutex;
+    std::vector<Command> queue;
+    juce::WaitableEvent event;
+
+public:
+    void push(Command cmd) {
+        {
+            std::lock_guard<std::mutex> lock(mutex);
+            queue.push_back(std::move(cmd));
+        }
+        event.signal();  // Wake up worker
+    }
+
+    bool waitAndPop(Command& cmd, int timeoutMs) {
+        if (!event.wait(timeoutMs)) return false;
+
+        std::lock_guard<std::mutex> lock(mutex);
+        if (queue.empty()) return false;
+
+        cmd = std::move(queue.front());
+        queue.erase(queue.begin());
+
+        if (!queue.empty()) {
+            event.signal();  // More items available
+        }
+        return true;
+    }
+};
+```
+
+**Pros:**
+
+- Unbounded capacity (grows as needed)
+- Multi-producer safe
+- Efficient blocking wait
+
+**Cons:**
+
+- Mutex contention (but only on queue, not business logic)
+- Dynamic allocation
+
+**Recommendation:** Use **Option B** for NetworkConnection - low event frequency, simplicity preferred
+
+### Worker Thread Pattern
+
+```cpp
+class ConnectionStageWorker : public juce::Thread {
+    CommandQueue& commandQueue;
+    NetworkConnectionState state;  // All private state
+
+public:
+    ConnectionStageWorker(CommandQueue& queue)
+        : juce::Thread("ConnectionWorker"), commandQueue(queue) {}
+
+    void run() override {
+        while (!threadShouldExit()) {
+            Command cmd;
+            if (commandQueue.waitAndPop(cmd, 100)) {  // 100ms timeout
+                processCommand(cmd);
+            }
+        }
+    }
+
+    void processCommand(const Command& cmd) {
+        switch (cmd.type) {
+            case Command::Connect:
+                handleConnect();
+                break;
+            case Command::CheckHeartbeat:
+                handleCheckHeartbeat();
+                break;
+            // ... etc
+        }
+    }
+
+    void handleConnect() {
+        // NO MUTEX NEEDED - Single thread owns state
+        state.currentState = State::Connecting;
+
+        // Perform handshake (blocking is OK here)
+        bool success = performHandshakeBlocking();
+
+        if (success) {
+            state.currentState = State::Connected;
+            state.lastHeartbeatTime = getCurrentTime();
+
+            // Notify listeners (async dispatch)
+            dispatchCallback([this] {
+                if (onStateChanged) {
+                    onStateChanged(State::Connecting, State::Connected);
+                }
+            });
+        }
+    }
+};
+```
+
+### Query Handling Without Blocking Production
+
+**Solution 1: Atomic Snapshot Variables**
+
+```cpp
+class NetworkConnection {
+    // Worker thread owns canonical state
+    ConnectionStageWorker worker;
+
+    // Atomic snapshots for queries (updated by worker)
+    std::atomic<State> stateSnapshot{State::Disconnected};
+    std::atomic<int64_t> heartbeatSnapshot{0};
+
+    // Worker updates snapshots after state changes
+    void workerUpdateSnapshots() {
+        stateSnapshot.store(canonicalState, std::memory_order_release);
+        heartbeatSnapshot.store(canonicalHeartbeat, std::memory_order_release);
+    }
+
+public:
+    // Query methods - NO LOCKING
+    State getState() const {
+        return stateSnapshot.load(std::memory_order_acquire);
+    }
+
+    int64_t getTimeSinceLastHeartbeat() const {
+        int64_t lastTime = heartbeatSnapshot.load(std::memory_order_acquire);
+        return getCurrentTime() - lastTime;
+    }
+};
+```
+
+**Solution 2: Query Command with Response**
+
+```cpp
+struct GetStateQuery {
+    juce::WaitableEvent responseReady;
+    State result;
+};
+
+State NetworkConnection::getState() const {
+    GetStateQuery query;
+    commandQueue.push(Command::GetState(&query));
+    query.responseReady.wait();  // Block until worker responds
+    return query.result;
+}
+
+// Worker thread:
+void handleGetStateQuery(GetStateQuery* query) {
+    query->result = currentState;
+    query->responseReady.signal();
+}
+```
+
+**Recommendation:** Use **Solution 1 (Atomic Snapshots)** for frequently-queried read-only data (state, heartbeat). Use
+**Solution 2 (Query Commands)** for complex queries requiring computation (device lists).
+
+---
+
+## Stage Decomposition
+
+### Identified Stages
+
+#### Stage 1: Connection Lifecycle Manager
+
+**Responsibilities:**
+
+- State machine transitions (Disconnected -> Connecting -> Connected -> Failed)
+- HTTP handshake execution
+- Socket lifecycle (bind, close)
+- Device list management
+
+**Input Events:**
+
+- `ConnectCommand`
+- `DisconnectCommand`
+- `HandshakeCompleteEvent` (internal)
+
+**Output Events:**
+
+- `StateChangedEvent` -> Notification Stage
+- `DevicesReceivedEvent` -> Notification Stage
+
+**State:**
+
+- `currentState`
+- `remoteNodeInfo`
+- `remoteDevices`
+- `httpClient`
+- `udpSocket`
+
+#### Stage 2: Heartbeat Manager
+
+**Responsibilities:**
+
+- Track last heartbeat timestamp
+- Respond to timeout checks
+- Send heartbeat packets (future)
+
+**Input Events:**
+
+- `NotifyHeartbeatReceivedEvent` (from UDP stage)
+- `CheckHeartbeatTimeoutCommand` (from HeartbeatMonitor)
+
+**Output Events:**
+
+- `HeartbeatTimeoutEvent` -> Connection Lifecycle Stage (trigger failure)
+
+**State:**
+
+- `lastHeartbeatTime`
+
+#### Stage 3: MIDI Message Transport
+
+**Responsibilities:**
+
+- Queue outgoing MIDI messages
+- Store received MIDI messages
+- Batch delivery to consumers
+
+**Input Events:**
+
+- `SendMidiCommand`
+- `MidiReceivedEvent` (from UDP stage)
+- `GetReceivedMessagesQuery`
+
+**Output Events:**
+
+- `UdpTransmitRequest` -> UDP Stage (future)
+
+**State:**
+
+- `receivedMessages`
+- `outgoingMessages`
+
+#### Stage 4: UDP I/O Stage (Future)
+
+**Responsibilities:**
+
+- Read from UDP socket
+- Write to UDP socket
+- Parse/serialize packets
+
+**Input Events:**
+
+- `UdpTransmitRequest`
+- Socket readability
+
+**Output Events:**
+
+- `MidiReceivedEvent` -> MIDI Transport Stage
+- `HeartbeatReceivedEvent` -> Heartbeat Stage
+
+**Recommendation:** Combine Stages 1 and 2 into single worker thread initially (low event rate). Stage 3 can share same
+worker. Stage 4 requires separate thread due to blocking I/O.
+
+---
+
+## Command/Event Design
+
+### Command Hierarchy
+
+```cpp
+namespace NetworkMidi {
+namespace Commands {
+
+// Base command
+struct Command {
+    enum Type {
+        Connect,
+        Disconnect,
+        CheckHeartbeat,
+        NotifyHeartbeat,
+        SendMidi,
+        GetState,
+        GetRemoteNode,
+        GetDevices,
+        Shutdown
+    };
+
+    Type type;
+
+    virtual ~Command() = default;
+};
+
+// Specific commands
+struct ConnectCommand : Command {
+    ConnectCommand() { type = Connect; }
+};
+
+struct DisconnectCommand : Command {
+    DisconnectCommand() { type = Disconnect; }
+};
+
+struct CheckHeartbeatCommand : Command {
+    CheckHeartbeatCommand() { type = CheckHeartbeat; }
+};
+
+struct NotifyHeartbeatCommand : Command {
+    NotifyHeartbeatCommand() { type = NotifyHeartbeat; }
+};
+
+struct SendMidiCommand : Command {
+    uint16_t deviceId;
+    std::vector<uint8_t> data;
+
+    SendMidiCommand(uint16_t id, std::vector<uint8_t> d)
+        : deviceId(id), data(std::move(d)) { type = SendMidi; }
+};
+
+// Query commands with synchronous response
+struct GetStateQuery : Command {
+    juce::WaitableEvent responseReady;
+    State result{State::Disconnected};
+
+    GetStateQuery() { type = GetState; }
+};
+
+struct GetRemoteNodeQuery : Command {
+    juce::WaitableEvent responseReady;
+    NodeInfo result;
+
+    GetRemoteNodeQuery() { type = GetRemoteNode; }
+};
+
+struct GetDevicesQuery : Command {
+    juce::WaitableEvent responseReady;
+    std::vector<DeviceInfo> result;
+
+    GetDevicesQuery() { type = GetDevices; }
+};
+
+} // namespace Commands
+} // namespace NetworkMidi
+```
+
+### Queue Implementation
+
+```cpp
+class NetworkConnectionQueue {
+public:
+    // Push command (called from any thread)
+    void pushCommand(std::unique_ptr<Commands::Command> cmd) {
+        std::lock_guard<std::mutex> lock(queueMutex);
+        commandQueue.push_back(std::move(cmd));
+        queueEvent.signal();
+    }
+
+    // Pop command (called from worker thread only)
+    std::unique_ptr<Commands::Command> popCommand(int timeoutMs) {
+        if (!queueEvent.wait(timeoutMs)) {
+            return nullptr;
+        }
+
+        std::lock_guard<std::mutex> lock(queueMutex);
+        if (commandQueue.empty()) {
+            return nullptr;
+        }
+
+        auto cmd = std::move(commandQueue.front());
+        commandQueue.pop_front();
+
+        if (!commandQueue.empty()) {
+            queueEvent.signal();  // More commands available
+        }
+
+        return cmd;
+    }
+
+private:
+    std::mutex queueMutex;
+    std::deque<std::unique_ptr<Commands::Command>> commandQueue;
+    juce::WaitableEvent queueEvent;
+};
+```
+
+---
+
+## Performance Considerations
+
+### Latency Analysis
+
+**Current (Mutex-based):**
+
+- Query latency: ~100ns (uncontended mutex) to 10ms+ (contended)
+- State change latency: Immediate (in-thread)
+
+**SEDA (Command-based):**
+
+- Query latency (atomic snapshot): ~10ns (atomic read)
+- Query latency (command): 100µs - 1ms (queue + worker processing)
+- State change latency: 100µs - 1ms (command queue delay)
+
+**Recommendation:** Use atomic snapshots for state/heartbeat queries. Accept command latency for infrequent operations (
+connect/disconnect).
+
+### Memory Overhead
+
+**Current:**
+
+- 3 mutexes × ~40 bytes = 120 bytes
+- Lock contention metadata: ~200 bytes (OS-dependent)
+
+**SEDA:**
+
+- Command queue: ~16KB (1024 commands × ~16 bytes each)
+- Worker thread stack: ~1MB (default)
+- Atomic snapshots: ~16 bytes
+
+**Impact:** Negligible for network application
+
+### Throughput
+
+**MIDI Messages:**
+
+- Current: ~100K msgs/sec (mutex overhead)
+- SEDA: ~1M msgs/sec (lock-free enqueue)
+
+**Heartbeat Checks:**
+
+- Current: ~10K checks/sec (mutex overhead)
+- SEDA: ~10M checks/sec (atomic read)
+
+---
+
+## Migration Strategy
+
+### Phase 1: Introduce Worker Thread
+
+- Create `ConnectionWorker` class extending `juce::Thread`
+- Implement command queue infrastructure
+- Keep existing mutex-based methods as public API
+- Commands internally post to queue, worker processes
+
+### Phase 2: Migrate State to Worker
+
+- Move `currentState`, `lastHeartbeatTime` ownership to worker
+- Update via atomic snapshots
+- Remove `stateMutex` and `heartbeatMutex`
+
+### Phase 3: Migrate Complex State
+
+- Move `remoteDevices`, `receivedMessages` to worker
+- Implement query commands for device list
+- Remove `messageMutex`
+
+### Phase 4: Add UDP Stage
+
+- Implement UDP receive loop in worker
+- Parse packets and dispatch to heartbeat/MIDI handlers
+
+---
+
+## Testing Strategy
+
+### Unit Testing
+
+```cpp
+TEST(NetworkConnectionSEDA, ConcurrentQueries) {
+    NetworkConnection conn(testNodeInfo);
+
+    // Spawn 100 reader threads
+    std::vector<std::thread> readers;
+    for (int i = 0; i < 100; ++i) {
+        readers.emplace_back([&conn] {
+            for (int j = 0; j < 1000; ++j) {
+                auto state = conn.getState();  // Should never deadlock
+                auto node = conn.getRemoteNode();
+                auto devices = conn.getRemoteDevices();
+            }
+        });
+    }
+
+    // Concurrent state changes
+    conn.connect();
+
+    for (auto& t : readers) {
+        t.join();  // Must complete without deadlock
+    }
+}
+```
+
+### Stress Testing
+
+```cpp
+TEST(NetworkConnectionSEDA, HighFrequencyCommands) {
+    NetworkConnection conn(testNodeInfo);
+
+    std::atomic<int> commandsProcessed{0};
+
+    // Send 100K commands from 10 threads
+    std::vector<std::thread> writers;
+    for (int i = 0; i < 10; ++i) {
+        writers.emplace_back([&] {
+            for (int j = 0; j < 10000; ++j) {
+                conn.checkHeartbeat();
+                ++commandsProcessed;
+            }
+        });
+    }
+
+    for (auto& t : writers) {
+        t.join();
+    }
+
+    // Allow worker to process
+    std::this_thread::sleep_for(std::chrono::seconds(1));
+
+    EXPECT_EQ(commandsProcessed, 100000);
+}
+```
+
+---
+
+## Architecture Review
+
+### Senior Architect Review Summary
+
+**Status:** ⚠️ **CONDITIONAL APPROVAL**
+
+The senior architecture review identified that **SEDA may be overengineered** for the current problem space, but
+provides value if:
+
+1. Deadlocks are actually occurring in production
+2. Performance profiling shows mutex contention
+3. Future scale requires 100+ connections
+
+### Alternative Recommendations
+
+#### Priority 1: Fix Callback-Under-Lock Pattern
+
+```cpp
+// BEFORE (risky):
+void NetworkConnection::setState(State newState) {
+    std::lock_guard<std::mutex> lock(stateMutex);
+    // ...
+    if (onStateChanged) {
+        onStateChanged(oldState, newState);  // ⚠️ Under lock
+    }
+}
+
+// AFTER (safe):
+void NetworkConnection::setState(State newState) {
+    auto callback = onStateChanged;  // Copy outside lock
+
+    {
+        std::lock_guard<std::mutex> lock(stateMutex);
+        // ... state transition ...
+    }
+
+    if (callback) {
+        callback(oldState, newState);  // ✅ Outside lock
+    }
+}
+```
+
+#### Priority 2: Selective Lock-Free Optimization
+
+```cpp
+// Replace message queue with lock-free variant
+#include <boost/lockfree/spsc_queue.hpp>
+
+class NetworkConnection {
+private:
+    boost::lockfree::spsc_queue<MidiMessage,
+                                 boost::lockfree::capacity<1024>> receivedMessages;
+};
+```
+
+#### Priority 3: Atomic Device List
+
+```cpp
+// Use RCU (Read-Copy-Update) for device lists
+class NetworkConnection {
+private:
+    std::atomic<std::shared_ptr<const std::vector<DeviceInfo>>> remoteDevices;
+
+    // Read (lock-free):
+    auto getDevices() const {
+        return remoteDevices.load(std::memory_order_acquire);
+    }
+
+    // Write (copy-on-write):
+    void updateDevices(const std::vector<DeviceInfo>& newDevices) {
+        auto devices = std::make_shared<std::vector<DeviceInfo>>(newDevices);
+        remoteDevices.store(devices, std::memory_order_release);
+    }
+};
+```
+
+### Deadlock Scenarios Resolved
+
+#### Scenario 1: HTTP Query During Handshake
+
+**Before (Mutex):**
+
+```
+HTTP Thread: getRemoteNode() acquires stateMutex
+Handshake Thread: performHandshake() tries to acquire stateMutex -> BLOCKED
+```
+
+**After (SEDA):**
+
+```
+HTTP Thread: getRemoteNode() reads atomic snapshot -> returns immediately
+Handshake Thread: processes ConnectCommand, updates snapshot, no locks
+```
+
+**Resolution:** ✅ No shared locks
+
+#### Scenario 2: Callback Reentrancy
+
+**Before (Mutex):**
+
+```
+setState() holds stateMutex
+  -> calls onStateChanged callback
+    -> callback calls getRemoteNode()
+      -> tries to acquire stateMutex -> DEADLOCK
+```
+
+**After (SEDA):**
+
+```
+Worker thread: processes command, updates state, dispatches callback via MessageManager
+Message thread: executes callback (worker thread already released everything)
+  -> callback calls getRemoteNode() -> reads atomic snapshot
+```
+
+**Resolution:** ✅ Callbacks on different thread, no reentrancy
+
+#### Scenario 3: HeartbeatMonitor Timer Contention
+
+**Before (Mutex):**
+
+```
+Timer Thread: getTimeSinceLastHeartbeat() acquires heartbeatMutex
+Worker Thread: notifyHeartbeatReceived() tries to acquire heartbeatMutex -> BLOCKED
+```
+
+**After (SEDA):**
+
+```
+Timer Thread: posts CheckHeartbeatCommand to queue -> returns
+Worker Thread: processes NotifyHeartbeatCommand, updates heartbeatSnapshot (atomic)
+Timer Thread: reads heartbeatSnapshot (atomic) -> no blocking
+```
+
+**Resolution:** ✅ Atomic updates, no mutex
+
+---
+
+## Decision Matrix
+
+| Approach                | Complexity | Performance | Deadlock Risk | Latency   | Effort |
+|-------------------------|------------|-------------|---------------|-----------|--------|
+| **Current (Mutex)**     | Low        | Good        | Medium        | <1μs      | 0h     |
+| **Mutex + Fixes**       | Low        | Good        | Low           | <1μs      | 2-4h   |
+| **Lock-Free Selective** | Medium     | Excellent   | Very Low      | <100ns    | 4-6h   |
+| **SEDA Full**           | High       | Excellent   | Zero          | 100μs-1ms | 8-14h  |
+| **Actor Model**         | Very High  | Excellent   | Zero          | 500μs-2ms | 40-60h |
+
+**Recommendation Order:**
+
+1. ✅ **Start with Mutex + Fixes** (quick win, low risk)
+2. ⏸️ **Monitor production for issues** (data-driven decision)
+3. 📊 **If deadlocks occur → SEDA** (proven need)
+4. 🚀 **If scale grows → Actor Model** (future v2.0)
+
+---
+
+## Appendices
+
+### Appendix A: JUCE Concurrency Primitives Reference
+
+| Primitive                           | Use Case                  | Thread Safety | Performance        |
+|-------------------------------------|---------------------------|---------------|--------------------|
+| `juce::Thread`                      | Worker thread base class  | N/A           | Medium (OS thread) |
+| `juce::WaitableEvent`               | Signaling between threads | Yes           | High (futex-based) |
+| `juce::AbstractFifo`                | Lock-free ring buffer     | SP/SC only    | Very High          |
+| `juce::CriticalSection`             | Mutex alternative         | Yes           | Medium             |
+| `juce::ReadWriteLock`               | Shared mutex              | Yes           | Medium             |
+| `juce::MessageManager::callAsync()` | Callback dispatch         | Yes           | Medium (queue)     |
+| `std::atomic`                       | Lock-free variables       | Yes           | Very High          |
+
+### Appendix B: Implementation Checklist
+
+**Core Infrastructure:**
+
+- [ ] Implement `NetworkConnectionQueue` with `juce::WaitableEvent`
+- [ ] Create `Command` base class and derived commands
+- [ ] Implement `ConnectionWorker : public juce::Thread`
+- [ ] Add command processing loop in `ConnectionWorker::run()`
+- [ ] Add shutdown mechanism (`ShutdownCommand`)
+
+**State Migration:**
+
+- [ ] Add atomic snapshot variables (`stateSnapshot`, `heartbeatSnapshot`)
+- [ ] Move `currentState` ownership to worker thread
+- [ ] Move `lastHeartbeatTime` ownership to worker thread
+- [ ] Update `getState()` to use atomic snapshot
+- [ ] Update `getTimeSinceLastHeartbeat()` to use atomic snapshot
+
+**Command Handlers:**
+
+- [ ] Implement `handleConnectCommand()` (calls `performHandshake()`)
+- [ ] Implement `handleDisconnectCommand()`
+- [ ] Implement `handleCheckHeartbeatCommand()`
+- [ ] Implement `handleNotifyHeartbeatCommand()`
+- [ ] Implement `handleSendMidiCommand()`
+
+**Query Handlers:**
+
+- [ ] Implement `handleGetStateQuery()` (optional - prefer atomic)
+- [ ] Implement `handleGetRemoteNodeQuery()`
+- [ ] Implement `handleGetDevicesQuery()`
+
+**Mutex Removal:**
+
+- [ ] Remove `stateMutex` from header
+- [ ] Remove `heartbeatMutex` from header
+- [ ] Remove all `std::lock_guard<std::mutex>` from implementation
+- [ ] Verify no shared mutable state remains
+
+**Testing:**
+
+- [ ] Unit tests for command queue
+- [ ] Unit tests for worker thread lifecycle
+- [ ] Concurrency tests (100+ threads querying)
+- [ ] Stress tests (100K+ commands)
+- [ ] Deadlock tests (TSAN/Helgrind)
+
+**Integration:**
+
+- [ ] Update `MeshManager` to work with new API
+- [ ] Update `HeartbeatMonitor` to work with new API
+- [ ] Update HTTP handlers to work with new API
+- [ ] Performance benchmarks (before/after comparison)
+
+### Appendix C: Reference Implementations
+
+**JUCE Forum Discussions:**
+
+- Lock-free audio processing patterns
+- Thread-safe state management
+
+**Academic Papers:**
+
+- "SEDA: An Architecture for Well-Conditioned, Scalable Internet Services" (Welsh et al., 2001)
+- "Wait-Free and Lock-Free Algorithms for Optimistic Concurrency Control" (Herlihy, 1993)
+
+**Open Source Examples:**
+
+- Chromium's task scheduling system (TaskRunner pattern)
+- Rust's `tokio` runtime (task-based concurrency)
+- JUCE's `AudioProcessorValueTreeState` (atomic parameters)
+
+---
+
+**Document Status:** Draft for Review
+**Next Steps:** Create implementation workplan in `./docs/1.0/seda/implementation/workplan.md`
+**Decision Point:** Evaluate mutex fixes vs. full SEDA based on production data
diff --git a/modules/juce/midi-server/network/mesh/MeshManager.cpp b/modules/juce/midi-server/network/mesh/MeshManager.cpp
index 2e9af6a..f3060be 100644
--- a/modules/juce/midi-server/network/mesh/MeshManager.cpp
+++ b/modules/juce/midi-server/network/mesh/MeshManager.cpp
@@ -144,16 +144,24 @@ std::vector<NodeInfo> MeshManager::getConnectedNodes() const
 //==============================================================================
 int MeshManager::getTotalDeviceCount() const
 {
+    std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Starting" << std::endl;
     int total = 0;
 
+    std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Getting all connections" << std::endl;
     auto connections = connectionPool.getAllConnections();
+    std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Got " << connections.size() << " connections" << std::endl;
 
-    for (const auto* connection : connections) {
+    for (size_t i = 0; i < connections.size(); ++i) {
+        auto* connection = connections[i];
+        std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Checking connection " << i << std::endl;
         if (connection && connection->getState() == NetworkConnection::State::Connected) {
+            std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Getting remote devices for connection " << i << std::endl;
             total += (int)connection->getRemoteDevices().size();
+            std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Got remote devices for connection " << i << std::endl;
         }
     }
 
+    std::cerr << "DEBUG: MeshManager::getTotalDeviceCount: Returning " << total << std::endl;
     return total;
 }
 
@@ -178,18 +186,25 @@ NetworkConnection* MeshManager::getConnection(const juce::Uuid& nodeId) const
 //==============================================================================
 MeshManager::MeshStatistics MeshManager::getStatistics() const
 {
+    std::cerr << "DEBUG: MeshManager::getStatistics: Starting" << std::endl;
     MeshStatistics stats;
 
+    std::cerr << "DEBUG: MeshManager::getStatistics: Getting pool stats" << std::endl;
     auto poolStats = connectionPool.getStatistics();
+    std::cerr << "DEBUG: MeshManager::getStatistics: Got pool stats" << std::endl;
     stats.totalNodes = poolStats.totalConnections;
     stats.connectedNodes = poolStats.connectedCount;
     stats.connectingNodes = poolStats.connectingCount;
     stats.failedNodes = poolStats.failedCount;
 
+    std::cerr << "DEBUG: MeshManager::getStatistics: Getting heartbeat stats" << std::endl;
     stats.heartbeatsSent = heartbeatMonitor->getHeartbeatsSent();
     stats.timeoutsDetected = heartbeatMonitor->getTimeoutsDetected();
+    std::cerr << "DEBUG: MeshManager::getStatistics: Got heartbeat stats" << std::endl;
 
+    std::cerr << "DEBUG: MeshManager::getStatistics: Getting total device count" << std::endl;
     stats.totalDevices = getTotalDeviceCount();
+    std::cerr << "DEBUG: MeshManager::getStatistics: Got total device count" << std::endl;
 
     return stats;
 }
diff --git a/modules/juce/midi-server/network/mesh/NetworkConnection.cpp b/modules/juce/midi-server/network/mesh/NetworkConnection.cpp
index 8e086d5..1bd1cf7 100644
--- a/modules/juce/midi-server/network/mesh/NetworkConnection.cpp
+++ b/modules/juce/midi-server/network/mesh/NetworkConnection.cpp
@@ -90,7 +90,8 @@ NodeInfo NetworkConnection::getRemoteNode() const
 //==============================================================================
 std::vector<DeviceInfo> NetworkConnection::getRemoteDevices() const
 {
-    std::lock_guard<std::mutex> lock(stateMutex);
+    // No mutex needed: remoteDevices is only written once during handshake
+    // and this is a read-only copy operation
     return remoteDevices;
 }
 
diff --git a/modules/juce/midi-server/network/transport/README.md b/modules/juce/midi-server/network/transport/README.md
index 2023706..3b5b10b 100644
--- a/modules/juce/midi-server/network/transport/README.md
+++ b/modules/juce/midi-server/network/transport/README.md
@@ -296,7 +296,7 @@ Offset | Size | Field              | Description
 
 ## References
 
-- [Workplan: Network MIDI Mesh](../../docs/1.0/implementation/mesh/workplan.md)
+- [Workplan: Network MIDI Mesh](../../docs/1.0/mesh/implementation/workplan.md)
 - [Phase 4 Implementation Report](PHASE4_REPORT.md)
 - [JUCE DatagramSocket Documentation](https://docs.juce.com/master/classDatagramSocket.html)
 


---

_**Agent (claude-sonnet-4-5-20250929)**_

feat(midi-server): add network handshake endpoint and debug logging for mesh connectivity

